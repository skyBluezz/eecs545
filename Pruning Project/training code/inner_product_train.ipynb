{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = 16\n",
    "layer_id = 'bn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "         #(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'D2':\n",
    "                layers += [nn.Dropout(p=0.2)]\n",
    "            elif x == 'D3':\n",
    "                layers += [nn.Dropout(p=0.3)]\n",
    "            elif x == 'D4':\n",
    "                layers += [nn.Dropout(p=0.4)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.BatchNorm2d(x)]\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG13').to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc_train(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 50000 train images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mass(net, l_index):\n",
    "    num_iter = 0\n",
    "    r = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1\n",
    "            if(num_iter == 40):\n",
    "                break\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            L_self = 0.0\n",
    "            L_mat = 0.0\n",
    "\n",
    "            for epoch_num in range(1):\n",
    "                out_features = net.features[0:l_index](inputs)\n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "#                 X_t = X_t - X_t.mean(2).reshape(out_features.shape[0], out_features.shape[1], 1)\n",
    "#                 X_t = torch.div(X_t, X_t.norm(dim=2).reshape(X_t.shape[0],X_t.shape[1],1) + 1e-10)\n",
    "                cov_mat = torch.matmul(X_t, X_t.permute(0,2,1))\n",
    "                L_mat = cov_mat.norm().pow(2)\n",
    "                \n",
    "                ident = (1 - torch.eye(out_features.shape[1])).to(device)\n",
    "                cov_mat = cov_mat*ident\n",
    "                L_self = cov_mat.norm().pow(2)\n",
    "                \n",
    "                r += 1 - L_self/L_mat\n",
    "\n",
    "            del L_self, L_mat, out_features\n",
    "            torch.cuda.empty_cache()\n",
    "        return r/num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar100_net.pth'\n",
    "# PATH = './tempnet1.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_corr = 0\n",
    "num_stop = 0\n",
    "for epoch in range(1):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_corr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat_corr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    num_stop = 0\n",
    "    print(n_index)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    net.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "    net.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "    \n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = (criterion(outputs, labels))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "            \n",
    "    loss_mat_corr[n_index] = running_loss\n",
    "    \n",
    "    net.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "# torch.save(loss_mat_corr, './decorr (features over samples)/loss_corrnet_bn_test_'+str(l_index)+'.pt')\n",
    "\n",
    "# torch.save(loss_mat_corr, './w_decorr/loss_corr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mat_corr = torch.load('./w_decorr/loss_corr_bn_train_'+str(l_index)+'.pt')\n",
    "torch.save(loss_mat_corr,'./temp'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    running_loss = 0.0\n",
    "    imp_corr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 50000):\n",
    "            break\n",
    "         \n",
    "    corrval = (np.corrcoef(imp_corr_bn.cpu().detach().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorrelated net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './inner_decorr/dnet_all.pth'\n",
    "PATH = './tempnet.pth'\n",
    "net_decorr = VGG('VGG13').to(device)\n",
    "net_decorr.load_state_dict(torch.load(PATH))\n",
    "net_decorr = net_decorr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net_decorr.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net_decorr.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "num_stop = 0\n",
    "loss_base_decorr = 0\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_decorr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "\n",
    "loss_mat_decorr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    print(n_index)\n",
    "    num_stop = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        net_decorr.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "        net_decorr.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "        outputs = net_decorr(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "            \n",
    "    loss_mat_decorr[n_index] = running_loss\n",
    "    \n",
    "    net_decorr.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net_decorr.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "torch.save(loss_mat_decorr, './inner_decorr/loss_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mat_decorr = torch.load('./w_decorr/loss_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    imp_decorr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_decorr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "        \n",
    "        imp_decorr_bn += (((net_decorr.features[l_index].weight.grad)*(net_decorr.features[l_index].weight.data)) + ((net_decorr.features[l_index].bias.grad)*(net_decorr.features[l_index].bias.data))).pow(2)\n",
    "    \n",
    "    corrval = (np.corrcoef(imp_decorr_bn.cpu().detach().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net-Slim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_corr = net.features[l_index].weight.data.clone()\n",
    "np.corrcoef(scale_corr.cpu().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_decorr = net_decorr.features[l_index].weight.data.clone().abs()\n",
    "np.corrcoef((scale_decorr).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 based pruning Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corr = net.features[l_index - 2].weight.data.clone()\n",
    "w_imp_corr = w_corr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_imp_corr.numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_decorr = net_decorr.features[l_index - 2].weight.data.clone()\n",
    "w_imp_decorr = w_decorr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "w_imp_decorr = (w_imp_decorr - w_imp_decorr.min())\n",
    "w_imp_decorr = w_imp_decorr/w_imp_decorr.max()\n",
    "np.corrcoef(w_imp_decorr.numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance plots TFO Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (Taylor FO)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (Taylor FO)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (Taylor FO)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance plots Netslim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_corr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_decorr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance plots L2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_corr.sort()[0].cpu().numpy()\n",
    "order = w_imp_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (L2)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_decorr.sort()[0].cpu().numpy()\n",
    "order = w_imp_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (L2)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VGG('VGG13').to(device)\n",
    "PATH = './cifar100_net.pth'\n",
    "# PATH = './inner_decorr/onet_all.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# net_d = VGG('VGG13').to(device)\n",
    "# PATH_d = './w_decorr/cifar100_w_decorr.pth'\n",
    "# net_d.load_state_dict(torch.load(PATH_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60.5200 %\n"
     ]
    }
   ],
   "source": [
    "cal_acc(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_temp = []\n",
    "\n",
    "for layer_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    \n",
    "    _, _, w_in, h_in = net.features[0:layer_index](torch.zeros(1,3,32,32).to(device)).shape\n",
    "    \n",
    "    c_out, c_in, w_f, h_f = net.features[layer_index-3].weight.shape\n",
    "    \n",
    "    l_temp.append((c_in*w_f*h_f)*(w_in*h_in)*c_out*(c_in*w_f*h_f*c_out**(1/5)))\n",
    "    \n",
    "    \n",
    "l_temp = np.array(l_temp)\n",
    "l_temp = l_temp/l_temp.sum()\n",
    "\n",
    "l_imp = {}\n",
    "i = 0\n",
    "for layer_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    \n",
    "    l_imp.update({layer_index : l_temp[i]})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel-wise Inner product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "l_inds = [0, 6, 10, 13, 17, 20, 24, 27, 31, 34, 35]\n",
    "\n",
    "for epoch in range(1):  \n",
    "    running_loss = 0.0\n",
    "    cov_loss = 0\n",
    "    num_iter = 0\n",
    "    av_cov_mass = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        out_features, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        for epoch_num in range(1):            \n",
    "            L_cov = 0.0\n",
    "            for ind in range(len(l_inds)-1):\n",
    "        \n",
    "                out_features = net.features[l_inds[ind]:l_inds[ind+1]](out_features)\n",
    "        \n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "#                 X_t = X_t - X_t.mean(2).reshape(out_features.shape[0], out_features.shape[1], 1)\n",
    "#                 X_t = torch.div(X_t, X_t.norm(dim=2).reshape(X_t.shape[0],X_t.shape[1],1) + 1e-15)\n",
    "                cov_mat = torch.matmul(X_t, X_t.permute(0,2,1))\n",
    "                L_cov += l_imp[layer_index]*(cov_mat - (torch.eye(out_features.shape[1])).to(device) / 1000).norm(1) / 128\n",
    "\n",
    "            outputs = net.classifier(out_features.reshape(out_features.shape[0], -1))\n",
    "            Lc = criterion(outputs, labels)\n",
    "            loss = Lc + 1e-5*L_cov\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            cov_loss += L_cov.item()\n",
    "        \n",
    "    print(\"Covariance loss: \" + str(cov_loss/num_iter))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance loss: 1288020.9359015345\n",
      "[1,   391] loss: 0.545\n",
      "Accuracy of the network on the 10000 test images: 56.0800 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "l_inds = [0, 2, 5, 9, 12, 16, 19, 23, 26, 30, 33]\n",
    "\n",
    "for epoch in range(1):  \n",
    "    running_loss = 0.0\n",
    "    cov_loss = 0\n",
    "    num_iter = 0\n",
    "    av_cov_mass = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        out_features, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        for epoch_num in range(1):            \n",
    "            L_cov = 0.0\n",
    "            for ind in range(len(l_inds)-1):\n",
    "        \n",
    "                out_features = net.features[l_inds[ind]:l_inds[ind+1]](out_features)\n",
    "        \n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "                x = net.features[l_inds[ind+1]].running_mean.data\n",
    "                mu_ij = torch.matmul(x.reshape(x.shape[0], 1), x.reshape(x.shape[0], 1).t())\n",
    "                \n",
    "                x = net.features[l_inds[ind+1]].running_var.data\n",
    "                sigma_ij = torch.matmul(x.reshape(x.shape[0], 1), x.reshape(x.shape[0], 1).t())\n",
    "\n",
    "                x = net.features[l_inds[ind+1]].bias.data\n",
    "                beta_ij = torch.matmul(x.reshape(x.shape[0], 1), x.reshape(x.shape[0], 1).t())\n",
    "\n",
    "                x = net.features[l_inds[ind+1]].weight.data\n",
    "                gamma_ij = torch.matmul(x.reshape(x.shape[0], 1), x.reshape(x.shape[0], 1).t())\n",
    "                \n",
    "                P = mu_ij - (beta_ij * (sigma_ij/ (gamma_ij + 1e-5)))\n",
    "                \n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "                cov_mat = torch.matmul(X_t, X_t.permute(0,2,1))\n",
    "                L_cov += (cov_mat - P).norm(1) / 128\n",
    "            \n",
    "            out_features = net.features[33:](out_features)\n",
    "            \n",
    "            outputs = net.classifier(out_features.reshape(out_features.shape[0], -1))\n",
    "            Lc = criterion(outputs, labels)\n",
    "            loss = Lc + 1e-7*L_cov\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            cov_loss += L_cov.item()\n",
    "        \n",
    "    print(\"Covariance loss: \" + str(cov_loss/num_iter))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_mass(net, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sigma_ij.abs()/(gamma_ij.abs()+1e-5)).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "l_inds = [0, 3, 6, 10, 13, 17, 20, 24, 27, 31, 34, 35]\n",
    "\n",
    "for epoch in range(1):  \n",
    "    running_loss = 0.0\n",
    "    cov_loss = 0\n",
    "    num_iter = 0\n",
    "    av_cov_mass = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        out_features, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        for epoch_num in range(1):            \n",
    "            L_cov = 0.0\n",
    "            for ind in range(len(l_inds)-1):\n",
    "        \n",
    "                out_features = net.features[l_inds[ind]:l_inds[ind+1]](out_features)\n",
    "        \n",
    "                X_t = out_features.permute(1,0,2,3).reshape(out_features.shape[1], -1)\n",
    "                cov_mat = torch.matmul(X_t, X_t.t())\n",
    "                L_cov += l_imp[layer_index]*(cov_mat*(1-torch.eye(out_features.shape[1])).to(device)).norm(1)\n",
    "            \n",
    "            out_features = net.features[l_inds[-2]:](out_features)\n",
    "            \n",
    "            outputs = net.classifier(out_features.reshape(out_features.shape[0], -1))\n",
    "            Lc = criterion(outputs, labels)\n",
    "            loss = Lc + 1e-5*L_cov\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            cov_loss += L_cov.item()\n",
    "        \n",
    "    print(\"Covariance loss: \" + str(cov_loss/num_iter))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_mass(net, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mass(net, l_index):\n",
    "    num_iter = 0\n",
    "    r = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1\n",
    "            if(num_iter == 40):\n",
    "                break\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            L_self = 0.0\n",
    "            L_mat = 0.0\n",
    "\n",
    "        for epoch_num in range(1):            \n",
    "            L_cov = 0.0\n",
    "            for layer_index in [34]: # [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "        \n",
    "                out_features = net.features[0:layer_index](inputs)\n",
    "        \n",
    "                X_t = out_features.permute(1,0,2,3).reshape(out_features.shape[1], -1)\n",
    "                cov_mat = torch.matmul(X_t, X_t.t())\n",
    "                L_mat = cov_mat.norm(1)\n",
    "                L_self += l_imp[layer_index]*(cov_mat - (torch.eye(out_features.shape[1])).to(device)).norm(1)\n",
    "                \n",
    "                r += 1 - L_self/L_mat\n",
    "\n",
    "            del L_self, L_mat, out_features\n",
    "            torch.cuda.empty_cache()\n",
    "        return r/num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VND regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "gamma = 1\n",
    "for epoch in range(3):  \n",
    "    running_loss = 0.0\n",
    "    cov_loss = 0\n",
    "    num_iter = 0\n",
    "    av_cov_mass = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        for epoch_num in range(1):            \n",
    "            VND = 0.0\n",
    "            for layer_index in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "        \n",
    "                w_tensors = net.features[layer_index].weight.data\n",
    "                w_mat = w_tensors.reshape(w_tensors.shape[0], -1)\n",
    "                G = torch.matmul(w_mat, w_mat.t())\n",
    "                \n",
    "                VND += (G - torch.eye(w_mat.shape[0]).to(device)).norm(1) # + torch.trace(G) #+ gamma*(w_mat.norm(1))\n",
    "#                 VND += torch.trace(G) - torch.log(torch.logdet(G + 1e-5*torch.eye(w_mat.shape[0]).to(device))) + gamma*(w_mat.norm(1))\n",
    "        \n",
    "            Lc = criterion(outputs, labels)\n",
    "            loss = Lc + 1e-4*VND\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            cov_loss += VND.item()\n",
    "        \n",
    "    print(\"Divergence loss: \" + str(cov_loss/num_iter))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    print(\"decorr: \", str(l), cal_mass(net, l).item())\n",
    "    print(\"orig \", str(l), cal_mass(net1, l).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './tempnet1.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
