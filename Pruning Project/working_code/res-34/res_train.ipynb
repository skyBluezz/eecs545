{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')\n",
    "net = ResNet34()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ortho train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building pretrained model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building pretrained model..')\n",
    "net_ortho = ResNet34()\n",
    "\n",
    "net_ortho = net_ortho.to(device)\n",
    "if device == 'cuda':\n",
    "    net_ortho = torch.nn.DataParallel(net_ortho)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint.\n",
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "net_ortho.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = [3,4,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp = {-1:{'conv1':net_ortho.module.bn1.bias.shape[0]}, 0:{}, 1:{}, 2:{}, 3:{}}\n",
    "\n",
    "mod_id = 0\n",
    "for module_id in [net_ortho.module.layer1, net_ortho.module.layer2, net_ortho.module.layer3, net_ortho.module.layer4]:\n",
    "    for b_id in range(num_blocks[mod_id]):\n",
    "        l_imp[mod_id].update({2*b_id: module_id[b_id].bn1.bias.shape[0]})\n",
    "        l_imp[mod_id].update({2*b_id+1: module_id[b_id].bn2.bias.shape[0]})\n",
    "    mod_id += 1\n",
    "    \n",
    "normalizer = 0\n",
    "for key, val in l_imp.items():\n",
    "    for key1, val1 in val.items():\n",
    "        normalizer += val1\n",
    "for key, val in l_imp.items():\n",
    "    for key1, val1 in val.items():\n",
    "        l_imp[key][key1] /= normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_ortho(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_ortho.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    angle_cost = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_ortho(inputs)\n",
    "\n",
    "        L_angle = 0\n",
    "\n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net_ortho.module.conv1.weight\n",
    "        params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += l_imp[-1]['conv1']*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        ### Conv_ind != 0 ###\n",
    "        mod_id = 0\n",
    "        for module_id in [net_ortho.module.layer1, net_ortho.module.layer2, net_ortho.module.layer3, net_ortho.module.layer4]:\n",
    "            for b_id in range(num_blocks[mod_id]):\n",
    "                w_mat = module_id[b_id].conv1.weight\n",
    "                params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "                angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(params.shape[0]).to(device)\n",
    "                L_angle += l_imp[mod_id][2*b_id]*(angle_mat).norm(1)\n",
    "\n",
    "                w_mat = module_id[b_id].conv2.weight\n",
    "                params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "                angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(params.shape[0]).to(device)\n",
    "                L_angle += l_imp[mod_id][2*b_id+1]*(angle_mat).norm(1)\n",
    "\n",
    "                try:\n",
    "                    w_mat = module_id[b_id].shortcut[0]\n",
    "                    params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "                    angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(params.shape[1]).to(device)\n",
    "                    L_angle += l_imp[mod_id][2*b_id]*(angle_mat).norm(1)\n",
    "                except:\n",
    "                    pass\n",
    "            mod_id += 1\n",
    "                \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-2)*(L_angle) + Lc\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (running_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    print(\"angle_cost: \", angle_cost/batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ortho(epoch):\n",
    "    global best_acc_ortho\n",
    "    net_ortho.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_ortho(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc_ortho:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_ortho': net_ortho.state_dict(),\n",
    "            'best_acc_ortho': acc\n",
    "        }\n",
    "        if not os.path.isdir('ortho_checkpoint'):\n",
    "            os.mkdir('ortho_checkpoint')\n",
    "        torch.save(state, './ortho_checkpoint/ortho_ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_diag():\n",
    "    \n",
    "    ### Conv_ind == 0 ###\n",
    "    w_mat = net_ortho.module.conv1.weight\n",
    "    params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "#     print(params.shape)\n",
    "    angle_mat = torch.matmul(torch.t(params), params) # - torch.eye(params.shape[1]).to(device)\n",
    "#     print(angle_mat.shape)\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "    ### Conv_ind != 0 ###\n",
    "    mod_id = 0\n",
    "    for module_id in [net_ortho.module.layer1, net_ortho.module.layer2, net_ortho.module.layer3, net_ortho.module.layer4]:\n",
    "        for b_id in range(num_blocks[mod_id]):\n",
    "            w_mat = module_id[b_id].conv1.weight\n",
    "            params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "#             print(params.shape)\n",
    "            angle_mat = torch.matmul(params, torch.t(params)) # - torch.eye(params.shape[0]).to(device)\n",
    "#             print(angle_mat.shape)\n",
    "            L_diag = (angle_mat.diag().norm(1))\n",
    "            L_angle = (angle_mat.norm(1))\n",
    "            print(L_diag.cpu()/L_angle.cpu())                \n",
    "\n",
    "            w_mat = module_id[b_id].conv2.weight\n",
    "            params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "#             print(params.shape)\n",
    "            angle_mat = torch.matmul(params, torch.t(params)) # - torch.eye(params.shape[0]).to(device)\n",
    "#             print(angle_mat.shape)\n",
    "            L_diag = (angle_mat.diag().norm(1))\n",
    "            L_angle = (angle_mat.norm(1))\n",
    "            print(L_diag.cpu()/L_angle.cpu())                \n",
    "\n",
    "            try:\n",
    "                w_mat = module_id[b_id].shortcut[0]\n",
    "                params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "                angle_mat = torch.matmul(params, torch.t(params)) # - torch.eye(params.shape[1]).to(device)\n",
    "                L_diag = (angle_mat.diag().norm(1))\n",
    "                L_angle = (angle_mat.norm(1))\n",
    "                print(L_diag.cpu()/L_angle.cpu())                \n",
    "            except:\n",
    "                pass\n",
    "        mod_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_ortho = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_ortho.parameters(), lr=1e-7, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp[-1]['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(ortho_start, ortho_start+1):\n",
    "    train_ortho(epoch)\n",
    "    test_ortho(epoch)\n",
    "    w_diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Resuming from checkpoint..')\n",
    "assert os.path.isdir('ortho_checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./ortho_checkpoint/ortho_ckpt.pth')\n",
    "net_ortho.load_state_dict(checkpoint['net_ortho'])\n",
    "best_acc_ortho = checkpoint['best_acc_ortho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ortho(epoch)\n",
    "#w_diag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
