{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "from utils import progress_bar\n",
    "from imp_baselines import*\n",
    "# import imp_baselines as baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(45),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./../data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./../data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, cfg[0], kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[0]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[0], cfg[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[2]),\n",
    "            nn.Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[3]),\n",
    "            nn.Conv2d(cfg[3], cfg[4], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[4]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[4] * 1 * 1, cfg[5]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[5], cfg[6]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(cfg[6], classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [64, 192, 384, 256, 256, 4096, 4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_corr = AlexNet(cfg).to(device)\n",
    "net_decorr = AlexNet(cfg).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './w_decorr/base_params/cifar100_net.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e8d9e34118c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./w_decorr/base_params/cifar100_net.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_corr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnet_corr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH_decorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./w_decorr/base_params/wnet_base.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './w_decorr/base_params/cifar100_net.pth'"
     ]
    }
   ],
   "source": [
    "PATH_corr = './w_decorr/base_params/cifar100_net.pth'\n",
    "net_dict = torch.load(PATH_corr)\n",
    "net_corr.load_state_dict(net_dict['net'])\n",
    "\n",
    "PATH_decorr = './w_decorr/base_params/wnet_base.pth'\n",
    "net_dict = torch.load(PATH_decorr)\n",
    "net_decorr.load_state_dict(net_dict['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,3,32,32))\n",
    "\n",
    "outputAt2 = net_corr.features[0:2](x)\n",
    "\n",
    "z = net_corr.features[2:6](y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(net_test):\n",
    "    net_test.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_test(inputs)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(100 * correct / total)\n",
    "        \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_importance_conv(net, l_index):\n",
    "    bias_base = net.features[l_index].bias.data.clone().detach()\n",
    "    av_corrval = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "\n",
    "    imp_norm = imp_corr_bn\n",
    "    \n",
    "    neuron_order = [np.linspace(0, imp_norm.shape[0]-1, imp_norm.shape[0]), imp_norm]\n",
    "    \n",
    "    return neuron_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_importance_linear(net, l_index):\n",
    "    bias_base = net.classifier[l_index].bias.data.clone().detach()\n",
    "    av_corrval = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        imp_corr_bn += (((net.classifier[l_index].weight.grad)*(net.classifier[l_index].weight.data)).sum(dim=1) + ((net.classifier[l_index].bias.grad)*(net.classifier[l_index].bias.data))).pow(2)\n",
    "\n",
    "    imp_norm = imp_corr_bn\n",
    "    \n",
    "    neuron_order = [np.linspace(0, imp_norm.shape[0]-1, imp_norm.shape[0]), imp_norm]\n",
    "    \n",
    "    return neuron_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time(net_acc):\n",
    "    net_acc.eval()\n",
    "    testsamp = torch.rand(1,3,32,32).to(device)\n",
    "    \n",
    "    for i in range(5):\n",
    "        net_acc(testsamp)    \n",
    "    t_end = 0\n",
    "    t_s = time.time()\n",
    "    for i in range(25):\n",
    "        net_acc(testsamp)\n",
    "        t_end += time.time() - t_s\n",
    "    \n",
    "    return (t_end / 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corr = cal_time(net_corr)\n",
    "t_decorr = cal_time(net_decorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Driven Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp_baselines import*\n",
    "import imp_baselines as baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'imp_baselines' from '/Users/skylergranatir/Desktop/EECS545/FinalProject/net-compression-master/working_code/alexnet/imp_baselines.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre-Train AlexNet on google drive\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './pretrained_alex.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net'])\n",
    "relu_layers = [1,5,9,12,15]\n",
    "#Prune with testset\n",
    "#Re-train / fine-Tune new Net\n",
    "#Re-Test new net\n",
    "#order and ratios \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working 1\n",
      "Sample  1\n"
     ]
    }
   ],
   "source": [
    "neuron_order = cal_importance_dataDriven(net, 1, num_stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample  1\n",
      "Sample  2\n",
      "Sample  3\n",
      "Sample  4\n",
      "Sample  5\n",
      "Sample  6\n",
      "Sample  7\n",
      "Sample  8\n",
      "Sample  9\n",
      "Sample  10\n",
      "[0.00484222 0.00610448 0.00763435 0.00427466 0.01091548 0.01008776\n",
      " 0.02645923 0.00392362 0.00666285 0.00564299 0.01130889 0.00822558\n",
      " 0.00533459 0.00756664 0.01671598 0.01022987 0.00484639 0.00589564\n",
      " 0.00417866 0.00997218 0.00421914 0.00387312 0.00745573 0.00579391\n",
      " 0.00372617 0.00931584 0.0274763  0.01244617 0.01432562 0.00591632\n",
      " 0.00622622 0.00516748 0.00925969 0.00560381 0.00793065 0.00854723\n",
      " 0.00578352 0.00892108 0.00454756 0.00690593 0.01211402 0.01118418\n",
      " 0.00838251 0.00828899 0.01179329 0.00394123 0.0043793  0.00863632\n",
      " 0.00708306 0.00699061 0.00735321 0.00720939 0.01014868 0.00505976\n",
      " 0.00643927 0.00348453 0.00599298 0.00828356 0.00741032 0.00842119\n",
      " 0.00374348 0.00789235 0.00489877 0.00334543]\n",
      "Neuron Order shape 2 64\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "num = 0\n",
    "size = net.features[0].weight.shape[0]\n",
    "aPoZ = np.zeros(size) #size of the conv layer before reLu...\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    num += 1 \n",
    "    if(num > 10):\n",
    "        break\n",
    "    print('Sample ', num)\n",
    "\n",
    "    ##for neuron in post relu...\n",
    "    ##if activation of weight is zero, add 1 to List at the weights index\n",
    "    ##should this be before back propogation?\n",
    "    l_id_output = net.features[0:2](inputs)  #output of the relu layer \n",
    "    # print(l_id_output.shape)\n",
    "\n",
    "    for a in range(0,l_id_output.shape[1]):\n",
    "           # print(l_id_output[:,a,:,:].shape)\n",
    "            sum_ = 0\n",
    "            sum_ = (0,np.count_nonzero(l_id_output.detach().numpy()[:,a,:,:] == 0))\n",
    "            aPoZ[a] += sum_[1]*0.001\n",
    "\n",
    "imp_corr_bn = 1/aPoZ #so that entries with highest aPoZ have lowest importance\n",
    "print(imp_corr_bn)\n",
    "neuron_order = [np.linspace(0, imp_corr_bn.shape[0]-1, imp_corr_bn.shape[0]), imp_corr_bn]\n",
    "print('Neuron Order shape', len(neuron_order), len(neuron_order[1]))\n",
    "#index in layer, layer index, importance.... m x 3 array.\n",
    "\n",
    "# #  imp_corr_bn = np.argsort(aPoZ)\n",
    "# imp_corr_bn = aPoZ\n",
    "# neuron_order = [np.linspace(0, imp_corr_bn.shape[0]-1, imp_corr_bn.shape[0]), imp_corr_bn]\n",
    "#index in layer, layer index, importance.... m x 3 array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 192, 384, 256, 256]\n",
      "(1152, 3)\n",
      "Working 1\n",
      "Sample  1\n",
      "Working 1\n",
      "Sample  1\n",
      "Working 1\n",
      "Sample  1\n",
      "Working 1\n",
      "Sample  1\n",
      "Working 1\n",
      "Sample  1\n",
      "[[0.00000000e+00 1.00000000e+00 4.97388728e-02]\n",
      " [1.00000000e+00 1.00000000e+00 6.20155036e-02]\n",
      " [2.00000000e+00 1.00000000e+00 7.34861866e-02]\n",
      " ...\n",
      " [2.53000000e+02 1.50000000e+01 1.89035928e+00]\n",
      " [2.54000000e+02 1.50000000e+01 1.44092214e+00]\n",
      " [2.55000000e+02 1.50000000e+01 1.45985401e+00]]\n"
     ]
    }
   ],
   "source": [
    "total_neurons = [net.features[l_id-1].weight.shape[0] for l_id in relu_layers]  #array with # neurons in each layer\n",
    "print((total_neurons))\n",
    "imp_matrix = np.zeros((sum(total_neurons),3)) #index in layer, layer index, importance\n",
    "print(imp_matrix.shape)\n",
    "a = []\n",
    "for i in range(len(total_neurons)):\n",
    "    a = np.concatenate((a, np.ones(total_neurons[i])*relu_layers[i]), axis=None)\n",
    "\n",
    "imp_matrix[:,1] = a\n",
    "num = 0\n",
    "i = 0\n",
    "\n",
    "for l_id in relu_layers:\n",
    "    neuron_order = cal_importance_dataDriven(net, l_id, num_stop=1)  ##CHANGE NUM_STOP TO 40 OR SOMETHING\n",
    "    imp_matrix[:,0][num:num+total_neurons[i]] = neuron_order[0]\n",
    "    imp_matrix[:,2][num:num+total_neurons[i]] = neuron_order[1]\n",
    "    num += total_neurons[i]\n",
    "    i += 1\n",
    "\n",
    "print(imp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratios 7\n",
      "orig_siz 7\n"
     ]
    }
   ],
   "source": [
    "### PRUNING\n",
    "orig_size = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "prune_ratio = 0.3\n",
    "imp_order, ratios = order_and_ratios(imp_matrix, prune_ratio)\n",
    "print('ratios', len(ratios))\n",
    "print('orig_siz', len(orig_size))\n",
    "net_pruned = pruner(net, imp_order, ratios, orig_size, net_type=0)\n",
    "##I changed pruner to concatenate the 4096,4096, thus they are unpruned.... Is this right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2 torch.Size([64])\n",
      "layer 4 torch.Size([192, 64, 3, 3])\n",
      "layer 8 torch.Size([384, 192, 3, 3])\n",
      "layer 10 torch.Size([384])\n",
      "layer 11 torch.Size([256, 384, 3, 3])\n",
      "layer 13 torch.Size([256])\n",
      "layer 14 torch.Size([256, 256, 3, 3])\n",
      "layer 16 torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    if(i%3==0 or i == 1 or i == 5 or i == 7):\n",
    "        continue\n",
    "    print('layer', i, net.features[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2 torch.Size([3])\n",
      "layer 4 torch.Size([3, 3, 3, 3])\n",
      "layer 8 torch.Size([2, 3, 3, 3])\n",
      "layer 10 torch.Size([2])\n",
      "layer 11 torch.Size([3, 2, 3, 3])\n",
      "layer 13 torch.Size([3])\n",
      "layer 14 torch.Size([3, 3, 3, 3])\n",
      "layer 16 torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    if(i%3==0 or i == 1 or i == 5 or i == 7):\n",
    "        continue\n",
    "    print('layer', i, net_pruned.features[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Train...\n",
    "optimizer = optim.SGD(net_pruned.parameters(), lr=1e-4, momentum=0.9)\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net_pruned(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Train...\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-4, momentum=0.9)\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test, want to get around 50.96\n",
    "cal_acc(net_pruned.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFO importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'rb') as f:\n",
    "    imp_order_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_corr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_corr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "    print(l_index)\n",
    "    if(l_index != 1 and l_index != 4):\n",
    "        nlist = cal_importance_conv(net_corr, l_index)\n",
    "    else:\n",
    "        nlist = cal_importance_linear(net_corr, l_index)\n",
    "    imp_order_corr = np.concatenate((imp_order_corr,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_corr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_w_decorr.pkl\", 'rb') as f:\n",
    "    imp_order_decorr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_decorr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "    print(l_index)\n",
    "    if(l_index != 1 and l_index != 4):\n",
    "        nlist = cal_importance_conv(net_decorr, l_index)\n",
    "    else:\n",
    "        nlist = cal_importance_linear(net_decorr, l_index)\n",
    "    imp_order_decorr = np.concatenate((imp_order_decorr,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/base_params/tfo_w_decorr.pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_decorr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_ratios(imp_order, prune_ratio):\n",
    "    imp_sort = np.argsort(imp_order[:,2])\n",
    "    temp_order = imp_order[imp_sort]\n",
    "\n",
    "    n_prune = int(prune_ratio * imp_order.shape[0])\n",
    "\n",
    "    prune_list = temp_order[0:n_prune]\n",
    "\n",
    "    imp_order_tfo = {}\n",
    "    ratios = []\n",
    "\n",
    "    for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "        nlist = temp_order[(temp_order[:,0] == l_index), 1].astype(int)\n",
    "        imp_order_tfo.update({l_index: nlist})\n",
    "        nlist = np.sort(prune_list[(prune_list[:,0] == l_index), 1].astype(int))\n",
    "        ratios.append(nlist.shape[0])\n",
    "    return imp_order_tfo, ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_p(prune_ratio, orig_size, save_cfg_corr=0, save_cfg=0):\n",
    "    cfg_list = []\n",
    "\n",
    "    for i in range(7):\n",
    "        cfg_list.append(orig_size[i] - prune_ratio[i])\n",
    "    \n",
    "    if(save_cfg == 1):\n",
    "        with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "\n",
    "    elif(save_cfg == 2):\n",
    "        with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "            \n",
    "    return cfg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4.]\n",
      " [4. 5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "def func(a,b):\n",
    "    return max(a,b)\n",
    "\n",
    "max(0,1)\n",
    "a = np.zeros((2,4))\n",
    "a[0,:] = [1,2,3,4]\n",
    "a[1,:] = [4,5,6,7]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0,1]\n",
    "m = 2\n",
    "a[0,:] = [-1,2,3,4]\n",
    "a[1,:] = [4,5,6,7]\n",
    "print(np.maximum(a,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruner(net, imp_order, prune_ratio, orig_size, net_type=0):\n",
    "    \n",
    "    if(net_type==1):\n",
    "        cfg = cfg_p(prune_ratio, orig_size, save_cfg=1)\n",
    "    elif(net_type==2):\n",
    "        cfg = cfg_p(prune_ratio, orig_size, save_cfg=2)\n",
    "    else:\n",
    "        cfg = cfg_p(prune_ratio, orig_size)\n",
    "        \n",
    "    cfg = np.concatenate((cfg,[4096,4096]), axis=None)\n",
    "    \n",
    "    net_pruned = AlexNet(cfg).to(device)\n",
    "    conv_layers = [2, 6, 10, 13, 16]\n",
    "    lin_layers = [1, 4]\n",
    "    \n",
    "    for l in range(len(conv_layers)):\n",
    "        if(l == 0):\n",
    "            n_c = prune_ratio[l]\n",
    "            order_c = np.sort(imp_order[conv_layers[l]][n_c:])\n",
    "            net_pruned.features[conv_layers[l]-2].weight.data = net.features[conv_layers[l]-2].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]-2].bias.data = net.features[conv_layers[l]-2].bias[order_c].data.detach().clone()\n",
    "\n",
    "            net_pruned.features[conv_layers[l]].weight.data = net.features[conv_layers[l]].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].bias.data = net.features[conv_layers[l]].bias[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].running_var.data = net.features[conv_layers[l]].running_var[order_c].detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].running_mean.data = net.features[conv_layers[l]].running_mean[order_c].detach().clone()    \n",
    "            continue\n",
    "        \n",
    "        n_p = prune_ratio[l-1]        \n",
    "        n_c = prune_ratio[l]\n",
    "\n",
    "        order_p = np.sort(imp_order[conv_layers[l-1]][n_p:])\n",
    "        order_c = np.sort(imp_order[conv_layers[l]][n_c:])\n",
    "        \n",
    "        net_pruned.features[conv_layers[l]-2].weight.data = net.features[conv_layers[l]-2].weight[order_c][:,order_p].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]-2].bias.data = net.features[conv_layers[l]-2].bias[order_c].detach().clone()\n",
    "\n",
    "        net_pruned.features[conv_layers[l]].weight.data = net.features[conv_layers[l]].weight[order_c].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]].bias.data = net.features[conv_layers[l]].bias[order_c].detach().clone()    \n",
    "        net_pruned.features[conv_layers[l]].running_var.data = net.features[conv_layers[l]].running_var[order_c].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]].running_mean.data = net.features[conv_layers[l]].running_mean[order_c].detach().clone()    \n",
    "\n",
    "    n_p = prune_ratio[4]        \n",
    "    n_c = prune_ratio[5]\n",
    "    order_p = np.sort(imp_order[conv_layers[4]][n_p:])\n",
    "    order_c = np.sort(imp_order[lin_layers[0]][n_c:])    \n",
    "    net_pruned.classifier[lin_layers[0]].weight.data = net.classifier[lin_layers[0]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[0]].bias.data = net.classifier[lin_layers[0]].bias[order_c].detach().clone()\n",
    "\n",
    "    n_p = prune_ratio[5]        \n",
    "    n_c = prune_ratio[6]\n",
    "    order_p = np.sort(imp_order[lin_layers[0]][n_p:])\n",
    "    order_c = np.sort(imp_order[lin_layers[1]][n_c:])    \n",
    "    net_pruned.classifier[lin_layers[1]].weight.data = net.classifier[lin_layers[1]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[1]].bias.data = net.classifier[lin_layers[1]].bias[order_c].detach().clone()\n",
    "\n",
    "    net_pruned.classifier[lin_layers[1]].weight.data = net.classifier[lin_layers[1]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[1]].bias.data = net.classifier[lin_layers[1]].bias[order_c].detach().clone()\n",
    "\n",
    "\n",
    "    n_classifier = prune_ratio[-1]\n",
    "    order_classifier = np.sort(imp_order[lin_layers[1]][n_classifier:])\n",
    "\n",
    "    net_pruned.classifier[6].weight.data = net.classifier[6].weight[:,order_classifier].detach().clone()\n",
    "    net_pruned.classifier[6].bias.data = net.classifier[6].bias.detach().clone()\n",
    "    \n",
    "    return net_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = []\n",
    "\n",
    "for i in [2, 6, 10, 13, 16]:\n",
    "    orig_size.append(net_corr.features[i].bias.shape[0])\n",
    "\n",
    "for i in [1, 4]:\n",
    "    orig_size.append(net_corr.classifier[i].bias.shape[0])\n",
    "    \n",
    "orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_corr, prune_ratio = order_and_ratios(imp_order_corr, 0.2)\n",
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load(PATH_corr)\n",
    "net_corr.load_state_dict(net_dict['net'])\n",
    "net_p = pruner(net_corr, order_corr, prune_ratio, orig_size, net_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p.eval()), cal_acc(net_corr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1 - cal_time(net_p) / t_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def net_p_train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_p.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def net_p_test(epoch):\n",
    "    global best_p_acc\n",
    "    global prune_iter\n",
    "    net_p.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_p(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_p_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_p': net_p.state_dict(),\n",
    "            'best_p_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('net_p_checkpoint'):\n",
    "            os.mkdir('net_p_checkpoint')\n",
    "        torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')\n",
    "        best_p_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    net_p_train(epoch)\n",
    "    net_p_test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load correlated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = torch.load('./net_p_checkpoint/ckpt1.pth')\n",
    "net_p.load_state_dict(net_dict['net_p'])\n",
    "best_p_acc = net_dict['best_p_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorrelated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = []\n",
    "for i in [2, 6, 10, 13, 16]:\n",
    "    orig_size.append(net_corr.features[i].bias.shape[0])\n",
    "for i in [1, 4]:\n",
    "    orig_size.append(net_corr.classifier[i].bias.shape[0])\n",
    "orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_decorr, prune_ratio = order_and_ratios(imp_order_decorr, 0.2)\n",
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load(PATH_decorr)\n",
    "net_decorr.load_state_dict(net_dict['net'])\n",
    "net_p_ortho = pruner(net_decorr, order_decorr, prune_ratio, orig_size, net_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cal_acc(net_p_ortho.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1 - cal_time(net_p_ortho) / t_decorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_p_test_ortho(epoch):\n",
    "    global best_p_ortho_acc\n",
    "    global prune_iter\n",
    "    net_p_ortho.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_p_ortho(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    print(acc)\n",
    "    if acc > best_p_ortho_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_p_ortho': net_p_ortho.state_dict(),\n",
    "            'best_p_ortho_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('ortho_p_checkpoint'):\n",
    "            os.mkdir('ortho_p_checkpoint')\n",
    "        torch.save(state, './ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')\n",
    "        best_p_ortho_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_p_train_ortho(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_p_ortho.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    angle_cost = 0.0\n",
    "            \n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p_ortho(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net_p_ortho.features[0].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_p_ortho.features[0].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += (l_imp[2])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "\n",
    "        ### Conv_ind != 0 ###\n",
    "        for conv_ind in [6, 10, 13, 16]:\n",
    "            w_mat = net_p_ortho.features[conv_ind-2].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            b_mat = net_p_ortho.features[conv_ind-2].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "            angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(w_mat.shape[0]).to(device)            \n",
    "            L_angle += (l_imp[conv_ind])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "    \n",
    "        ### lin_ind = 1 ###        \n",
    "        w_mat = net_p_ortho.classifier[1].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_p_ortho.classifier[1].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += (l_imp[1])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        ### lin_ind = 4 ###        \n",
    "        w_mat = net_p_ortho.classifier[4].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_p_ortho.classifier[4].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(params.shape[0]).to(device)\n",
    "        L_angle += (l_imp[4])*(angle_mat).norm(1) #.norm().pow(2))        \n",
    "        \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-1)*(L_angle) + Lc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (running_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp = {}\n",
    "\n",
    "for conv_ind in [2, 6, 10, 13, 16]:\n",
    "    l_imp.update({conv_ind: net_p_ortho.features[conv_ind].bias.shape[0]})\n",
    "    \n",
    "for lin_ind in [1, 4]:\n",
    "    l_imp.update({lin_ind: net_p_ortho.classifier[lin_ind].bias.shape[0]})\n",
    "    \n",
    "normalizer = 0\n",
    "for key, val in l_imp.items():\n",
    "    normalizer += val\n",
    "for key, val in l_imp.items():\n",
    "    l_imp[key] = val / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_imp[0] = 0 #l_imp[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p_ortho.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_ortho_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    net_p_train_ortho(epoch)\n",
    "    net_p_test_ortho(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load decorrelated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = torch.load('./ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')\n",
    "net_p_ortho.load_state_dict(net_dict['net_p_ortho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate orthogonality of filters in pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conv_ind == 0 ###\n",
    "w_mat = net_p_ortho.features[0].weight\n",
    "w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "b_mat = net_p_ortho.features[0].bias\n",
    "b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "angle_mat = torch.matmul(torch.t(params), params)# - torch.eye(params.shape[1]).to(device)\n",
    "L_diag = (angle_mat.diag().norm(1))\n",
    "L_angle = (angle_mat.norm(1))\n",
    "print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "for conv_ind in [6, 10, 13, 16]:\n",
    "    w_mat = net_p_ortho.features[conv_ind-2].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    b_mat = net_p_ortho.features[conv_ind-2].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "    angle_mat = torch.matmul(params, torch.t(params))# - torch.eye(w_mat.shape[0]).to(device)            \n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "### lin_ind = 1 ###        \n",
    "w_mat = net_p_ortho.classifier[1].weight\n",
    "w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "b_mat = net_p_ortho.classifier[1].bias\n",
    "b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "angle_mat = torch.matmul(torch.t(params), params)# - torch.eye(params.shape[1]).to(device)\n",
    "L_diag = (angle_mat.diag().norm(1))\n",
    "L_angle = (angle_mat.norm(1))\n",
    "print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "### lin_ind = 4 ###        \n",
    "w_mat = net_p_ortho.classifier[4].weight\n",
    "w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "b_mat = net_p_ortho.classifier[4].bias\n",
    "b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "angle_mat = torch.matmul(params, torch.t(params))# - torch.eye(params.shape[0]).to(device)\n",
    "L_diag = (angle_mat.diag().norm(1))\n",
    "L_angle = (angle_mat.norm(1))\n",
    "print(L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "#     imp_order_p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_p.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "    print(l_index)\n",
    "    if(l_index != 1 and l_index != 4):\n",
    "        nlist = cal_importance_conv(net_p, l_index)\n",
    "    else:\n",
    "        nlist = cal_importance_linear(net_p, l_index)\n",
    "    imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network '''\n",
    "# with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p\"+str(prune_iter)+\".pkl\", 'rb') as f:\n",
    "#     imp_order_p_ortho = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_p_ortho.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p_ortho = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "    print(l_index)\n",
    "    if(l_index != 1 and l_index != 4):\n",
    "        nlist = cal_importance_conv(net_p_ortho, l_index)\n",
    "    else:\n",
    "        nlist = cal_importance_linear(net_p_ortho, l_index)\n",
    "    imp_order_p_ortho = np.concatenate((imp_order_p_ortho,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "\n",
    "with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p_ortho\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p_ortho, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruned network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network '''\n",
    "# orig_size = []\n",
    "# for i in [2, 6, 10, 13, 16]:\n",
    "#     orig_size.append(net_p.features[i].bias.shape[0])\n",
    "# for i in [1, 4]:\n",
    "#     orig_size.append(net_p.classifier[i].bias.shape[0])\n",
    "# orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network '''\n",
    "orig_size = []\n",
    "for i in [2, 6, 10, 13, 16]:\n",
    "    orig_size.append(net_p_ortho.features[i].bias.shape[0])\n",
    "for i in [1, 4]:\n",
    "    orig_size.append(net_p_ortho.classifier[i].bias.shape[0])\n",
    "orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network '''\n",
    "# order_p, prune_ratio = order_and_ratios(imp_order_p, 0.1)\n",
    "# prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network '''\n",
    "order_p, prune_ratio = order_and_ratios(imp_order_p_ortho, 0.1)\n",
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ''' Correlated network pruning '''\n",
    "# net_p1 = pruner(net_p, order_p, prune_ratio, orig_size, net_type=1)\n",
    "\n",
    "# print(\"Accs:\", cal_acc(net_p1.eval()), cal_acc(net_p.eval()))\n",
    "# print(\"Time:\", cal_time(net_p1), t_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network pruning '''\n",
    "net_p1_ortho = pruner(net_p_ortho, order_p, prune_ratio, orig_size, net_type=2)\n",
    "\n",
    "print(\"Accs:\", cal_acc(net_p1_ortho.eval()), cal_acc(net_p_ortho.eval()))\n",
    "print(\"Time:\", cal_time(net_p1_ortho), t_decorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network saving '''\n",
    "# net_p = net_p1\n",
    "\n",
    "# print('Saving..')\n",
    "# state = {\n",
    "#     'net_p': net_p.state_dict(),\n",
    "#     'best_p_acc': cal_acc(net_p.eval())\n",
    "# }\n",
    "# if not os.path.isdir('net_p_checkpoint'):\n",
    "#     os.mkdir('net_p_checkpoint')\n",
    "# torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network saving '''\n",
    "net_p_ortho = net_p1_ortho\n",
    "\n",
    "print('Saving..')\n",
    "state = {\n",
    "    'net_p_ortho': net_p_ortho.state_dict(),\n",
    "    'best_p_ortho_acc': cal_acc(net_p_ortho.eval())\n",
    "}\n",
    "if not os.path.isdir('ortho_p_checkpoint'):\n",
    "    os.mkdir('ortho_p_checkpoint')\n",
    "torch.save(state, './ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network loading '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(1)+\".pkl\", 'rb') as f:\n",
    "#     cfg_p1 = pickle.load(f)\n",
    "    \n",
    "# net_p = AlexNet(cfg_p1).to(device)\n",
    "# PATH = './net_p_checkpoint/ckpt'+str(1)+'.pth'\n",
    "# net_p.load_state_dict(torch.load(PATH)['net_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network loading '''\n",
    "with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter\"+str(1)+\".pkl\", 'rb') as f:\n",
    "    cfg_p1 = pickle.load(f)\n",
    "\n",
    "net_p_ortho = AlexNet(cfg_p1).to(device)\n",
    "PATH = './ortho_p_checkpoint/ortho_ckpt'+str(2)+'.pth'\n",
    "net_p_ortho.load_state_dict(torch.load(PATH)['net_p_ortho'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p_ortho.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPS calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_p_ortho, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "    \n",
    "# with torch.cuda.device(0):\n",
    "#     flops, params = get_model_complexity_info(net_p, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', flops))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_decorr, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "\n",
    "# with torch.cuda.device(0):\n",
    "#     flops, params = get_model_complexity_info(net_corr, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', flops))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
