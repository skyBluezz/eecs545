{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "from utils import progress_bar\n",
    "from imp_baselines import*\n",
    "# import imp_baselines as baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(45),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./../data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./../data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequentiaal(\n",
    "            nn.Conv2d(3, cfg[0], kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[0]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[0], cfg[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[2]),\n",
    "            nn.Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[3]),\n",
    "            nn.Conv2d(cfg[3], cfg[4], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[4]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[4] * 1 * 1, cfg[5]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[5], cfg[6]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(cfg[6], classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [64, 192, 384, 256, 256, 4096, 4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet(cfg).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_corr = './w_decorr/base_params/cifar100_net.pth'\n",
    "net_dict = torch.load(PATH_corr)\n",
    "net.load_state_dict(net_dict['net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(net_test):\n",
    "    net_test.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_test(inputs)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(100 * correct / total)\n",
    "        \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Driven Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp_baselines import*\n",
    "import imp_baselines as baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre-Train AlexNet on google drive\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './pretrained_alex.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net'])\n",
    "relu_layers = [1,5,9,12,15]\n",
    "classifier_relu_layers = [2, 5]\n",
    "#Prune with testset\n",
    "#Re-train / fine-Tune new Net\n",
    "#Re-Test new net\n",
    "#order and ratios \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_importance_DD(net, relu_layers, classifier_relu_layers):\n",
    "\n",
    "    total_neurons = [net.features[l_id-1].weight.shape[0] for l_id in relu_layers]  #array with # neurons in each layer\n",
    "    for l_id in classifier_relu_layers:\n",
    "        total_neurons.append(net.classifier[l_id-1].weight.shape[0])\n",
    "    print((total_neurons))\n",
    "    imp_matrix = np.zeros((sum(total_neurons),3)) #index in layer, layer index, importance\n",
    "    print(imp_matrix.shape)\n",
    "    a = []\n",
    "    for i in range(len(total_neurons)-2):\n",
    "        print(i)\n",
    "        a = np.concatenate((a, np.ones(total_neurons[i])*(relu_layers[i] + 1)), axis=None)\n",
    "    for i in range(len(total_neurons)-2, len(total_neurons)):\n",
    "        print(i)\n",
    "        a = np.concatenate((a, np.ones(total_neurons[i])*(classifier_relu_layers[i-5] - 1)), axis=None)\n",
    "\n",
    "    imp_matrix[:,0] = a\n",
    "    num = 0\n",
    "    i = 0\n",
    "\n",
    "    for l_id in relu_layers:\n",
    "        neuron_order = cal_importance_dataDriven_conv(net, l_id, num_stop=100)  ##CHANGE NUM_STOP TO 40 OR SOMETHING\n",
    "        imp_matrix[:,1][num:num+total_neurons[i]] = neuron_order[0]\n",
    "        imp_matrix[:,2][num:num+total_neurons[i]] = neuron_order[1] / total_neurons[i]\n",
    "        num += total_neurons[i]\n",
    "        i += 1\n",
    "\n",
    "    for l_id in classifier_relu_layers:\n",
    "        neuron_order = cal_importance_dataDriven_linear(net, l_id, num_stop=100)  ##CHANGE NUM_STOP TO 40 OR SOMETHING\n",
    "        imp_matrix[:,1][num:num+total_neurons[i]] = neuron_order[0]\n",
    "        imp_matrix[:,2][num:num+total_neurons[i]] = neuron_order[1] / total_neurons[i]\n",
    "        num += total_neurons[i]\n",
    "        i += 1\n",
    "    return imp_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFO importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_ratios(imp_order, prune_ratio):\n",
    "    imp_sort = np.argsort(imp_order[:,2])\n",
    "    temp_order = imp_order[imp_sort]\n",
    "\n",
    "    n_prune = int(prune_ratio * imp_order.shape[0])\n",
    "\n",
    "    prune_list = temp_order[0:n_prune]\n",
    "\n",
    "    imp_order_tfo = {}\n",
    "    ratios = []\n",
    "\n",
    "    for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "        nlist = temp_order[(temp_order[:,0] == l_index), 1].astype(int)\n",
    "        imp_order_tfo.update({l_index: nlist})\n",
    "        nlist = np.sort(prune_list[(prune_list[:,0] == l_index), 1].astype(int))\n",
    "        ratios.append(nlist.shape[0])\n",
    "    return imp_order_tfo, ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_p(prune_ratio, orig_size, save_cfg_corr=0, save_cfg=0):\n",
    "    cfg_list = []\n",
    "\n",
    "    for i in range(7):\n",
    "        cfg_list.append(orig_size[i] - prune_ratio[i])\n",
    "\n",
    "    if(save_cfg == 1):\n",
    "        with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "    \n",
    "    return cfg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [1, 1, 384, 1, 256, 1, 4096]\n",
    "nets = AlexNet(vector).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruner(net, imp_order, prune_ratio, orig_size, net_type=0):\n",
    "    if(net_type==1):\n",
    "        cfg = cfg_p(prune_ratio, orig_size, save_cfg=1)\n",
    "    else:\n",
    "        cfg = cfg_p(prune_ratio, orig_size)        \n",
    "    print('Size cfg ',cfg)\n",
    "    ###########\n",
    "    ######New.  Enforce at least 1 neuron########\n",
    "    for i in range(len(cfg)):\n",
    "        if(cfg[i] == 0):\n",
    "            cfg[i] = 1\n",
    "    #########\n",
    "    ##########\n",
    "    print('Size cfg ',cfg)\n",
    "\n",
    "    net_pruned = AlexNet(cfg).to(device)\n",
    "    conv_layers = [2, 6, 10, 13, 16]\n",
    "    lin_layers = [1, 4]\n",
    "    \n",
    "    for l in range(len(conv_layers)):\n",
    "        if(l == 0):\n",
    "            n_c = prune_ratio[l]\n",
    "            order_c = np.sort(imp_order[conv_layers[l]][n_c:])\n",
    "            net_pruned.features[conv_layers[l]-2].weight.data = net.features[conv_layers[l]-2].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]-2].bias.data = net.features[conv_layers[l]-2].bias[order_c].data.detach().clone()\n",
    "\n",
    "            net_pruned.features[conv_layers[l]].weight.data = net.features[conv_layers[l]].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].bias.data = net.features[conv_layers[l]].bias[order_c].data.detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].running_var.data = net.features[conv_layers[l]].running_var[order_c].detach().clone()\n",
    "            net_pruned.features[conv_layers[l]].running_mean.data = net.features[conv_layers[l]].running_mean[order_c].detach().clone()    \n",
    "            continue\n",
    "        \n",
    "        n_p = prune_ratio[l-1]        \n",
    "        n_c = prune_ratio[l]\n",
    "\n",
    "        order_p = np.sort(imp_order[conv_layers[l-1]][n_p:])\n",
    "        order_c = np.sort(imp_order[conv_layers[l]][n_c:])\n",
    "        \n",
    "        net_pruned.features[conv_layers[l]-2].weight.data = net.features[conv_layers[l]-2].weight[order_c][:,order_p].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]-2].bias.data = net.features[conv_layers[l]-2].bias[order_c].detach().clone()\n",
    "\n",
    "        net_pruned.features[conv_layers[l]].weight.data = net.features[conv_layers[l]].weight[order_c].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]].bias.data = net.features[conv_layers[l]].bias[order_c].detach().clone()    \n",
    "        net_pruned.features[conv_layers[l]].running_var.data = net.features[conv_layers[l]].running_var[order_c].detach().clone()\n",
    "        net_pruned.features[conv_layers[l]].running_mean.data = net.features[conv_layers[l]].running_mean[order_c].detach().clone()    \n",
    "\n",
    "    n_p = prune_ratio[4]        \n",
    "    n_c = prune_ratio[5]\n",
    "    order_p = np.sort(imp_order[conv_layers[4]][n_p:])\n",
    "    order_c = np.sort(imp_order[lin_layers[0]][n_c:])    \n",
    "    net_pruned.classifier[lin_layers[0]].weight.data = net.classifier[lin_layers[0]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[0]].bias.data = net.classifier[lin_layers[0]].bias[order_c].detach().clone()\n",
    "\n",
    "    n_p = prune_ratio[5]        \n",
    "    n_c = prune_ratio[6]\n",
    "    order_p = np.sort(imp_order[lin_layers[0]][n_p:])\n",
    "    order_c = np.sort(imp_order[lin_layers[1]][n_c:])    \n",
    "    net_pruned.classifier[lin_layers[1]].weight.data = net.classifier[lin_layers[1]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[1]].bias.data = net.classifier[lin_layers[1]].bias[order_c].detach().clone()\n",
    "\n",
    "    net_pruned.classifier[lin_layers[1]].weight.data = net.classifier[lin_layers[1]].weight[order_c][:,order_p].detach().clone()\n",
    "    net_pruned.classifier[lin_layers[1]].bias.data = net.classifier[lin_layers[1]].bias[order_c].detach().clone()\n",
    "\n",
    "\n",
    "    n_classifier = prune_ratio[-1]\n",
    "    order_classifier = np.sort(imp_order[lin_layers[1]][n_classifier:])\n",
    "\n",
    "    net_pruned.classifier[6].weight.data = net.classifier[6].weight[:,order_classifier].detach().clone()\n",
    "    net_pruned.classifier[6].bias.data = net.classifier[6].bias.detach().clone()\n",
    "    \n",
    "    return net_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRUNING\n",
    "orig_size = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "prune_ratio = 0.6\n",
    "imp_order, ratios = order_and_ratios(imp_matrix, prune_ratio)\n",
    "print('ratios', (ratios))\n",
    "print('orig_siz', (orig_size))\n",
    "net_p = pruner(net, imp_order, ratios, orig_size, net_type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune 60%\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './pretrained_alex.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net'])\n",
    "relu_layers = [1,5,9,12,15]\n",
    "classifier_relu_layers = [2, 5]\n",
    "orig_size = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "prune_ratio = 0.6 ###You can change here\n",
    "imp_matrix = calc_importance_DD(net, relu_layers, classifier_relu_layers)\n",
    "print(imp_matrix)\n",
    "\n",
    "imp_order, ratios = order_and_ratios(imp_matrix, prune_ratio)\n",
    "net_p = pruner(net, imp_order, ratios, orig_size, net_type=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "for i in range(10):\n",
    "    net_p_train(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    net_p_train(200)\n",
    "##Prune Ratio of 0.6 w/ no iterative pruning, accuracy = 46.1%.  Decreases by 4%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Iterative pruning...\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './pretrained_alex.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net'])\n",
    "relu_layers = [1,5,9,12,15]\n",
    "classifier_relu_layers = [2, 5]\n",
    "orig_size = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "prune_ratios = [0.30,0.25,0.20] #This is 58% pruned\n",
    "\n",
    "for prune_ratio in prune_ratios:\n",
    "    imp_matrix = calc_importance_DD(net, relu_layers, classifier_relu_layers)\n",
    "    imp_order, ratios = order_and_ratios(imp_matrix, prune_ratio)\n",
    "    net_p = pruner(net, imp_order, ratios, orig_size, net_type=0)\n",
    "    orig_size = ratios\n",
    "    for i in range(30):\n",
    "        net_p_train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run more \n",
    "###Save the pruned network \n",
    "\n",
    "\n",
    "#RESULTS\n",
    "#60% ITERATIVE PRUNING, 50.781% accuracy\n",
    "#60% intitial pruning, 46.43% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    net_p_train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.96"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = [64,158,264,203,144,138,106]\n",
    "#cfg = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './ortho_p_final_ckpts/nets/ortho_ckpt12.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net_p_ortho'])\n",
    "cal_acc(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.33"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = [64,157,212,186,113,138,100]\n",
    "#cfg = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './ortho_p_final_ckpts/nets/ortho_ckpt13.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net_p_ortho'])\n",
    "cal_acc(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "0\n",
      "Saving.....................]  Step: 21s567ms | Tot: 0ms | Loss: 1.494 | Acc: 64.062% (82/12 1/391 \n",
      "1\n",
      "Saving.....................]  Step: 379ms | Tot: 380ms | Loss: 1.599 | Acc: 61.328% (157/25 2/391 \n",
      "2\n",
      "Saving.....................]  Step: 346ms | Tot: 726ms | Loss: 1.719 | Acc: 57.031% (219/38 3/391 \n",
      "3\n",
      "Saving.....................]  Step: 358ms | Tot: 1s85ms | Loss: 1.808 | Acc: 53.125% (272/51 4/391 \n",
      "4\n",
      "Saving.....................]  Step: 356ms | Tot: 1s442ms | Loss: 1.887 | Acc: 50.781% (325/64 5/391 \n",
      "5\n",
      "6[>........................]  Step: 368ms | Tot: 1s810ms | Loss: 1.972 | Acc: 48.698% (374/76 6/391 \n",
      "7[>........................]  Step: 350ms | Tot: 2s160ms | Loss: 2.016 | Acc: 47.545% (426/89 7/391 \n",
      "8[>........................]  Step: 379ms | Tot: 2s539ms | Loss: 2.035 | Acc: 47.070% (482/102 8/391 \n",
      "9[>........................]  Step: 367ms | Tot: 2s907ms | Loss: 2.049 | Acc: 46.528% (536/115 9/391 \n",
      "10>........................]  Step: 341ms | Tot: 3s248ms | Loss: 2.055 | Acc: 46.250% (592/128 10/391 \n",
      "11>........................]  Step: 350ms | Tot: 3s599ms | Loss: 2.078 | Acc: 45.597% (642/140 11/391 \n",
      "12>........................]  Step: 356ms | Tot: 3s955ms | Loss: 2.100 | Acc: 44.661% (686/153 12/391 \n",
      "13>........................]  Step: 351ms | Tot: 4s306ms | Loss: 2.117 | Acc: 44.411% (739/166 13/391 \n",
      "14>........................]  Step: 344ms | Tot: 4s651ms | Loss: 2.148 | Acc: 43.638% (782/179 14/391 \n",
      "15>........................]  Step: 343ms | Tot: 4s994ms | Loss: 2.167 | Acc: 43.333% (832/192 15/391 \n",
      "16>........................]  Step: 364ms | Tot: 5s359ms | Loss: 2.187 | Acc: 42.871% (878/204 16/391 \n",
      "17=>.......................]  Step: 383ms | Tot: 5s742ms | Loss: 2.200 | Acc: 42.509% (925/217 17/391 \n",
      "18=>.......................]  Step: 362ms | Tot: 6s104ms | Loss: 2.208 | Acc: 42.231% (973/230 18/391 \n",
      "19=>.......................]  Step: 362ms | Tot: 6s466ms | Loss: 2.212 | Acc: 41.859% (1018/243 19/391 \n",
      "20=>.......................]  Step: 348ms | Tot: 6s815ms | Loss: 2.217 | Acc: 41.953% (1074/256 20/391 \n",
      "21=>.......................]  Step: 378ms | Tot: 7s193ms | Loss: 2.226 | Acc: 41.741% (1122/268 21/391 \n",
      "22=>.......................]  Step: 352ms | Tot: 7s546ms | Loss: 2.228 | Acc: 41.797% (1177/281 22/391 \n",
      "23=>.......................]  Step: 344ms | Tot: 7s890ms | Loss: 2.234 | Acc: 41.440% (1220/294 23/391 \n",
      "24=>.......................]  Step: 370ms | Tot: 8s260ms | Loss: 2.240 | Acc: 41.178% (1265/307 24/391 \n",
      "25=>.......................]  Step: 365ms | Tot: 8s625ms | Loss: 2.245 | Acc: 40.938% (1310/320 25/391 \n",
      "26=>.......................]  Step: 369ms | Tot: 8s994ms | Loss: 2.249 | Acc: 40.535% (1349/332 26/391 \n",
      "27=>.......................]  Step: 369ms | Tot: 9s364ms | Loss: 2.249 | Acc: 40.307% (1393/345 27/391 \n",
      "28=>.......................]  Step: 376ms | Tot: 9s740ms | Loss: 2.258 | Acc: 39.927% (1431/358 28/391 \n",
      "29=>.......................]  Step: 391ms | Tot: 10s131ms | Loss: 2.262 | Acc: 39.709% (1474/371 29/391 \n",
      "30=>.......................]  Step: 382ms | Tot: 10s513ms | Loss: 2.269 | Acc: 39.635% (1522/384 30/391 \n",
      "31=>.......................]  Step: 367ms | Tot: 10s881ms | Loss: 2.281 | Acc: 39.491% (1567/396 31/391 \n",
      "32=>.......................]  Step: 376ms | Tot: 11s257ms | Loss: 2.288 | Acc: 39.307% (1610/409 32/391 \n",
      "33==>......................]  Step: 375ms | Tot: 11s632ms | Loss: 2.286 | Acc: 39.299% (1660/422 33/391 \n",
      "34==>......................]  Step: 368ms | Tot: 12s1ms | Loss: 2.289 | Acc: 39.223% (1707/435 34/391 \n",
      "35==>......................]  Step: 368ms | Tot: 12s370ms | Loss: 2.294 | Acc: 39.107% (1752/448 35/391 \n",
      "36==>......................]  Step: 365ms | Tot: 12s736ms | Loss: 2.298 | Acc: 39.019% (1798/460 36/391 \n",
      "37==>......................]  Step: 362ms | Tot: 13s98ms | Loss: 2.298 | Acc: 39.105% (1852/473 37/391 \n",
      "38==>......................]  Step: 359ms | Tot: 13s457ms | Loss: 2.305 | Acc: 38.939% (1894/486 38/391 \n",
      "39==>......................]  Step: 364ms | Tot: 13s822ms | Loss: 2.310 | Acc: 38.842% (1939/499 39/391 \n",
      "40==>......................]  Step: 361ms | Tot: 14s183ms | Loss: 2.315 | Acc: 38.828% (1988/512 40/391 \n",
      "41==>......................]  Step: 363ms | Tot: 14s546ms | Loss: 2.316 | Acc: 38.758% (2034/524 41/391 \n",
      "42==>......................]  Step: 364ms | Tot: 14s911ms | Loss: 2.316 | Acc: 38.783% (2085/537 42/391 \n",
      "43==>......................]  Step: 362ms | Tot: 15s274ms | Loss: 2.320 | Acc: 38.626% (2126/550 43/391 \n",
      "44==>......................]  Step: 358ms | Tot: 15s632ms | Loss: 2.318 | Acc: 38.672% (2178/563 44/391 \n",
      "45==>......................]  Step: 372ms | Tot: 16s4ms | Loss: 2.320 | Acc: 38.646% (2226/576 45/391 \n",
      "46==>......................]  Step: 373ms | Tot: 16s378ms | Loss: 2.320 | Acc: 38.587% (2272/588 46/391 \n",
      "47==>......................]  Step: 382ms | Tot: 16s761ms | Loss: 2.319 | Acc: 38.664% (2326/601 47/391 \n",
      "48===>.....................]  Step: 400ms | Tot: 17s161ms | Loss: 2.318 | Acc: 38.688% (2377/614 48/391 \n",
      "49===>.....................]  Step: 397ms | Tot: 17s559ms | Loss: 2.320 | Acc: 38.632% (2423/627 49/391 \n",
      "50===>.....................]  Step: 382ms | Tot: 17s941ms | Loss: 2.323 | Acc: 38.562% (2468/640 50/391 \n",
      "51===>.....................]  Step: 392ms | Tot: 18s334ms | Loss: 2.322 | Acc: 38.618% (2521/652 51/391 \n",
      "52===>.....................]  Step: 370ms | Tot: 18s705ms | Loss: 2.321 | Acc: 38.672% (2574/665 52/391 \n",
      "53===>.....................]  Step: 368ms | Tot: 19s74ms | Loss: 2.323 | Acc: 38.723% (2627/678 53/391 \n",
      "54===>.....................]  Step: 361ms | Tot: 19s435ms | Loss: 2.325 | Acc: 38.686% (2674/691 54/391 \n",
      "55===>.....................]  Step: 363ms | Tot: 19s799ms | Loss: 2.324 | Acc: 38.594% (2717/704 55/391 \n",
      "56===>.....................]  Step: 356ms | Tot: 20s155ms | Loss: 2.323 | Acc: 38.616% (2768/716 56/391 \n",
      "57===>.....................]  Step: 356ms | Tot: 20s512ms | Loss: 2.322 | Acc: 38.624% (2818/729 57/391 \n",
      "58===>.....................]  Step: 355ms | Tot: 20s868ms | Loss: 2.327 | Acc: 38.551% (2862/742 58/391 \n",
      "59===>.....................]  Step: 360ms | Tot: 21s229ms | Loss: 2.324 | Acc: 38.626% (2917/755 59/391 \n",
      "60===>.....................]  Step: 358ms | Tot: 21s587ms | Loss: 2.327 | Acc: 38.529% (2959/768 60/391 \n",
      "61===>.....................]  Step: 359ms | Tot: 21s947ms | Loss: 2.331 | Acc: 38.473% (3004/780 61/391 \n",
      "62===>.....................]  Step: 371ms | Tot: 22s319ms | Loss: 2.329 | Acc: 38.533% (3058/793 62/391 \n",
      "63===>.....................]  Step: 366ms | Tot: 22s685ms | Loss: 2.328 | Acc: 38.542% (3108/806 63/391 \n",
      "64====>....................]  Step: 365ms | Tot: 23s50ms | Loss: 2.329 | Acc: 38.489% (3153/819 64/391 \n",
      "65====>....................]  Step: 367ms | Tot: 23s418ms | Loss: 2.327 | Acc: 38.546% (3207/832 65/391 \n",
      "66====>....................]  Step: 366ms | Tot: 23s784ms | Loss: 2.326 | Acc: 38.554% (3257/844 66/391 \n",
      "67====>....................]  Step: 360ms | Tot: 24s145ms | Loss: 2.330 | Acc: 38.549% (3306/857 67/391 \n",
      "68====>....................]  Step: 393ms | Tot: 24s538ms | Loss: 2.332 | Acc: 38.523% (3353/870 68/391 \n",
      "69====>....................]  Step: 378ms | Tot: 24s916ms | Loss: 2.333 | Acc: 38.542% (3404/883 69/391 \n",
      "70====>....................]  Step: 370ms | Tot: 25s286ms | Loss: 2.333 | Acc: 38.482% (3448/896 70/391 \n",
      "71====>....................]  Step: 369ms | Tot: 25s656ms | Loss: 2.334 | Acc: 38.424% (3492/908 71/391 \n",
      "72====>....................]  Step: 376ms | Tot: 26s33ms | Loss: 2.333 | Acc: 38.509% (3549/921 72/391 \n",
      "73====>....................]  Step: 373ms | Tot: 26s406ms | Loss: 2.333 | Acc: 38.538% (3601/934 73/391 \n",
      "74====>....................]  Step: 373ms | Tot: 26s780ms | Loss: 2.332 | Acc: 38.535% (3650/947 74/391 \n",
      "75====>....................]  Step: 366ms | Tot: 27s146ms | Loss: 2.332 | Acc: 38.500% (3696/960 75/391 \n",
      "76====>....................]  Step: 374ms | Tot: 27s521ms | Loss: 2.332 | Acc: 38.559% (3751/972 76/391 \n",
      "77====>....................]  Step: 369ms | Tot: 27s890ms | Loss: 2.335 | Acc: 38.464% (3791/985 77/391 \n",
      "78====>....................]  Step: 373ms | Tot: 28s263ms | Loss: 2.335 | Acc: 38.391% (3833/998 78/391 \n",
      "79====>....................]  Step: 356ms | Tot: 28s620ms | Loss: 2.336 | Acc: 38.370% (3880/1011 79/391 \n",
      "80=====>...................]  Step: 366ms | Tot: 28s986ms | Loss: 2.336 | Acc: 38.369% (3929/1024 80/391 \n",
      "81=====>...................]  Step: 360ms | Tot: 29s346ms | Loss: 2.337 | Acc: 38.349% (3976/1036 81/391 \n",
      "82=====>...................]  Step: 392ms | Tot: 29s739ms | Loss: 2.337 | Acc: 38.386% (4029/1049 82/391 \n",
      "83=====>...................]  Step: 364ms | Tot: 30s104ms | Loss: 2.337 | Acc: 38.375% (4077/1062 83/391 \n",
      "84=====>...................]  Step: 375ms | Tot: 30s480ms | Loss: 2.337 | Acc: 38.346% (4123/1075 84/391 \n",
      "85=====>...................]  Step: 361ms | Tot: 30s841ms | Loss: 2.338 | Acc: 38.336% (4171/1088 85/391 \n",
      "86=====>...................]  Step: 360ms | Tot: 31s201ms | Loss: 2.337 | Acc: 38.363% (4223/1100 86/391 \n",
      "87=====>...................]  Step: 362ms | Tot: 31s563ms | Loss: 2.335 | Acc: 38.461% (4283/1113 87/391 \n",
      "88=====>...................]  Step: 371ms | Tot: 31s935ms | Loss: 2.334 | Acc: 38.450% (4331/1126 88/391 \n",
      "89=====>...................]  Step: 361ms | Tot: 32s297ms | Loss: 2.335 | Acc: 38.422% (4377/1139 89/391 \n",
      "90=====>...................]  Step: 372ms | Tot: 32s669ms | Loss: 2.336 | Acc: 38.359% (4419/1152 90/391 \n",
      "91=====>...................]  Step: 372ms | Tot: 33s42ms | Loss: 2.335 | Acc: 38.393% (4472/1164 91/391 \n",
      "92=====>...................]  Step: 372ms | Tot: 33s414ms | Loss: 2.333 | Acc: 38.451% (4528/1177 92/391 \n",
      "93=====>...................]  Step: 386ms | Tot: 33s801ms | Loss: 2.333 | Acc: 38.466% (4579/1190 93/391 \n",
      "94=====>...................]  Step: 411ms | Tot: 34s212ms | Loss: 2.335 | Acc: 38.423% (4623/1203 94/391 \n",
      "95======>..................]  Step: 377ms | Tot: 34s590ms | Loss: 2.336 | Acc: 38.388% (4668/1216 95/391 \n",
      "96======>..................]  Step: 374ms | Tot: 34s964ms | Loss: 2.338 | Acc: 38.403% (4719/1228 96/391 \n",
      "97======>..................]  Step: 376ms | Tot: 35s341ms | Loss: 2.340 | Acc: 38.370% (4764/1241 97/391 \n",
      "98======>..................]  Step: 401ms | Tot: 35s743ms | Loss: 2.346 | Acc: 38.313% (4806/1254 98/391 \n",
      "99======>..................]  Step: 410ms | Tot: 36s153ms | Loss: 2.345 | Acc: 38.352% (4860/1267 99/391 \n",
      "100=====>..................]  Step: 406ms | Tot: 36s560ms | Loss: 2.344 | Acc: 38.383% (4913/1280 100/391 \n",
      "101=====>..................]  Step: 414ms | Tot: 36s974ms | Loss: 2.344 | Acc: 38.382% (4962/1292 101/391 \n",
      "102=====>..................]  Step: 399ms | Tot: 37s374ms | Loss: 2.346 | Acc: 38.335% (5005/1305 102/391 \n",
      "103=====>..................]  Step: 400ms | Tot: 37s774ms | Loss: 2.344 | Acc: 38.395% (5062/1318 103/391 \n",
      "104=====>..................]  Step: 427ms | Tot: 38s202ms | Loss: 2.343 | Acc: 38.416% (5114/1331 104/391 \n",
      "105=====>..................]  Step: 389ms | Tot: 38s591ms | Loss: 2.345 | Acc: 38.341% (5153/1344 105/391 \n",
      "106=====>..................]  Step: 383ms | Tot: 38s975ms | Loss: 2.344 | Acc: 38.414% (5212/1356 106/391 \n",
      "107=====>..................]  Step: 371ms | Tot: 39s346ms | Loss: 2.345 | Acc: 38.383% (5257/1369 107/391 \n",
      "108=====>..................]  Step: 364ms | Tot: 39s711ms | Loss: 2.346 | Acc: 38.354% (5302/1382 108/391 \n",
      "109=====>..................]  Step: 384ms | Tot: 40s95ms | Loss: 2.346 | Acc: 38.360% (5352/1395 109/391 \n",
      "110=====>..................]  Step: 375ms | Tot: 40s471ms | Loss: 2.346 | Acc: 38.317% (5395/1408 110/391 \n",
      "111======>.................]  Step: 376ms | Tot: 40s847ms | Loss: 2.346 | Acc: 38.316% (5444/1420 111/391 \n",
      "112======>.................]  Step: 372ms | Tot: 41s219ms | Loss: 2.349 | Acc: 38.239% (5482/1433 112/391 \n",
      "113======>.................]  Step: 376ms | Tot: 41s596ms | Loss: 2.349 | Acc: 38.212% (5527/1446 113/391 \n",
      "114======>.................]  Step: 371ms | Tot: 41s968ms | Loss: 2.350 | Acc: 38.165% (5569/1459 114/391 \n",
      "115======>.................]  Step: 365ms | Tot: 42s334ms | Loss: 2.349 | Acc: 38.207% (5624/1472 115/391 \n",
      "116======>.................]  Step: 366ms | Tot: 42s700ms | Loss: 2.349 | Acc: 38.221% (5675/1484 116/391 \n",
      "117======>.................]  Step: 371ms | Tot: 43s71ms | Loss: 2.348 | Acc: 38.228% (5725/1497 117/391 \n",
      "118======>.................]  Step: 363ms | Tot: 43s435ms | Loss: 2.348 | Acc: 38.242% (5776/1510 118/391 \n",
      "119======>.................]  Step: 360ms | Tot: 43s795ms | Loss: 2.349 | Acc: 38.242% (5825/1523 119/391 \n",
      "120======>.................]  Step: 371ms | Tot: 44s166ms | Loss: 2.350 | Acc: 38.268% (5878/1536 120/391 \n",
      "121======>.................]  Step: 358ms | Tot: 44s524ms | Loss: 2.349 | Acc: 38.301% (5932/1548 121/391 \n",
      "122======>.................]  Step: 364ms | Tot: 44s889ms | Loss: 2.349 | Acc: 38.320% (5984/1561 122/391 \n",
      "123======>.................]  Step: 373ms | Tot: 45s262ms | Loss: 2.350 | Acc: 38.275% (6026/1574 123/391 \n",
      "124======>.................]  Step: 374ms | Tot: 45s637ms | Loss: 2.351 | Acc: 38.256% (6072/1587 124/391 \n",
      "125======>.................]  Step: 364ms | Tot: 46s2ms | Loss: 2.350 | Acc: 38.275% (6124/1600 125/391 \n",
      "126======>.................]  Step: 369ms | Tot: 46s371ms | Loss: 2.351 | Acc: 38.294% (6176/1612 126/391 \n",
      "127=======>................]  Step: 374ms | Tot: 46s746ms | Loss: 2.352 | Acc: 38.306% (6227/1625 127/391 \n",
      "128=======>................]  Step: 367ms | Tot: 47s113ms | Loss: 2.352 | Acc: 38.287% (6273/1638 128/391 \n",
      "129=======>................]  Step: 368ms | Tot: 47s482ms | Loss: 2.351 | Acc: 38.281% (6321/1651 129/391 \n",
      "130=======>................]  Step: 375ms | Tot: 47s857ms | Loss: 2.351 | Acc: 38.347% (6381/1664 130/391 \n",
      "131=======>................]  Step: 366ms | Tot: 48s224ms | Loss: 2.351 | Acc: 38.335% (6428/1676 131/391 \n",
      "132=======>................]  Step: 361ms | Tot: 48s585ms | Loss: 2.350 | Acc: 38.346% (6479/1689 132/391 \n",
      "133=======>................]  Step: 363ms | Tot: 48s949ms | Loss: 2.350 | Acc: 38.346% (6528/1702 133/391 \n",
      "134=======>................]  Step: 358ms | Tot: 49s307ms | Loss: 2.350 | Acc: 38.316% (6572/1715 134/391 \n",
      "135=======>................]  Step: 368ms | Tot: 49s675ms | Loss: 2.349 | Acc: 38.328% (6623/1728 135/391 \n",
      "136=======>................]  Step: 372ms | Tot: 50s47ms | Loss: 2.350 | Acc: 38.327% (6672/1740 136/391 \n",
      "137=======>................]  Step: 367ms | Tot: 50s415ms | Loss: 2.349 | Acc: 38.338% (6723/1753 137/391 \n",
      "138=======>................]  Step: 365ms | Tot: 50s780ms | Loss: 2.348 | Acc: 38.377% (6779/1766 138/391 \n",
      "139=======>................]  Step: 364ms | Tot: 51s145ms | Loss: 2.348 | Acc: 38.349% (6823/1779 139/391 \n",
      "140=======>................]  Step: 363ms | Tot: 51s509ms | Loss: 2.348 | Acc: 38.371% (6876/1792 140/391 \n",
      "141=======>................]  Step: 357ms | Tot: 51s866ms | Loss: 2.350 | Acc: 38.331% (6918/1804 141/391 \n",
      "142========>...............]  Step: 369ms | Tot: 52s236ms | Loss: 2.351 | Acc: 38.325% (6966/1817 142/391 \n",
      "143========>...............]  Step: 361ms | Tot: 52s597ms | Loss: 2.352 | Acc: 38.276% (7006/1830 143/391 \n",
      "144========>...............]  Step: 371ms | Tot: 52s968ms | Loss: 2.354 | Acc: 38.222% (7045/1843 144/391 \n",
      "145========>...............]  Step: 367ms | Tot: 53s335ms | Loss: 2.356 | Acc: 38.179% (7086/1856 145/391 \n",
      "146========>...............]  Step: 367ms | Tot: 53s702ms | Loss: 2.356 | Acc: 38.206% (7140/1868 146/391 \n",
      "147========>...............]  Step: 358ms | Tot: 54s61ms | Loss: 2.355 | Acc: 38.186% (7185/1881 147/391 \n",
      "148========>...............]  Step: 374ms | Tot: 54s435ms | Loss: 2.355 | Acc: 38.213% (7239/1894 148/391 \n",
      "149========>...............]  Step: 351ms | Tot: 54s787ms | Loss: 2.357 | Acc: 38.145% (7275/1907 149/391 \n",
      "150========>...............]  Step: 356ms | Tot: 55s144ms | Loss: 2.357 | Acc: 38.104% (7316/1920 150/391 \n",
      "151========>...............]  Step: 363ms | Tot: 55s508ms | Loss: 2.358 | Acc: 38.074% (7359/1932 151/391 \n",
      "152========>...............]  Step: 363ms | Tot: 55s871ms | Loss: 2.356 | Acc: 38.127% (7418/1945 152/391 \n",
      "153========>...............]  Step: 370ms | Tot: 56s241ms | Loss: 2.357 | Acc: 38.113% (7464/1958 153/391 \n",
      "154========>...............]  Step: 372ms | Tot: 56s614ms | Loss: 2.358 | Acc: 38.119% (7514/1971 154/391 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155========>...............]  Step: 368ms | Tot: 56s982ms | Loss: 2.358 | Acc: 38.145% (7568/1984 155/391 \n",
      "156========>...............]  Step: 365ms | Tot: 57s347ms | Loss: 2.357 | Acc: 38.156% (7619/1996 156/391 \n",
      "157========>...............]  Step: 363ms | Tot: 57s710ms | Loss: 2.358 | Acc: 38.157% (7668/2009 157/391 \n",
      "158=========>..............]  Step: 357ms | Tot: 58s68ms | Loss: 2.357 | Acc: 38.148% (7715/2022 158/391 \n",
      "159=========>..............]  Step: 360ms | Tot: 58s428ms | Loss: 2.357 | Acc: 38.134% (7761/2035 159/391 \n",
      "160=========>..............]  Step: 362ms | Tot: 58s791ms | Loss: 2.358 | Acc: 38.130% (7809/2048 160/391 \n",
      "161=========>..............]  Step: 370ms | Tot: 59s161ms | Loss: 2.359 | Acc: 38.116% (7855/2060 161/391 \n",
      "162=========>..............]  Step: 359ms | Tot: 59s520ms | Loss: 2.359 | Acc: 38.132% (7907/2073 162/391 \n",
      "163=========>..............]  Step: 366ms | Tot: 59s887ms | Loss: 2.358 | Acc: 38.181% (7966/2086 163/391 \n",
      "164=========>..............]  Step: 370ms | Tot: 1m258ms | Loss: 2.358 | Acc: 38.224% (8024/2099 164/391 \n",
      "165=========>..............]  Step: 364ms | Tot: 1m622ms | Loss: 2.358 | Acc: 38.191% (8066/2112 165/391 \n",
      "166=========>..............]  Step: 365ms | Tot: 1m988ms | Loss: 2.358 | Acc: 38.197% (8116/2124 166/391 \n",
      "167=========>..............]  Step: 363ms | Tot: 1m1s | Loss: 2.359 | Acc: 38.178% (8161/2137 167/391 \n",
      "168=========>..............]  Step: 367ms | Tot: 1m1s | Loss: 2.360 | Acc: 38.128% (8199/2150 168/391 \n",
      "169=========>..............]  Step: 365ms | Tot: 1m2s | Loss: 2.361 | Acc: 38.101% (8242/2163 169/391 \n",
      "170=========>..............]  Step: 365ms | Tot: 1m2s | Loss: 2.360 | Acc: 38.116% (8294/2176 170/391 \n",
      "171=========>..............]  Step: 360ms | Tot: 1m2s | Loss: 2.361 | Acc: 38.085% (8336/2188 171/391 \n",
      "172=========>..............]  Step: 363ms | Tot: 1m3s | Loss: 2.361 | Acc: 38.100% (8388/2201 172/391 \n",
      "173=========>..............]  Step: 360ms | Tot: 1m3s | Loss: 2.360 | Acc: 38.119% (8441/2214 173/391 \n",
      "174==========>.............]  Step: 371ms | Tot: 1m3s | Loss: 2.359 | Acc: 38.191% (8506/2227 174/391 \n",
      "175==========>.............]  Step: 359ms | Tot: 1m4s | Loss: 2.361 | Acc: 38.138% (8543/2240 175/391 \n",
      "176==========>.............]  Step: 362ms | Tot: 1m4s | Loss: 2.360 | Acc: 38.130% (8590/2252 176/391 \n",
      "177==========>.............]  Step: 351ms | Tot: 1m4s | Loss: 2.360 | Acc: 38.153% (8644/2265 177/391 \n",
      "178==========>.............]  Step: 358ms | Tot: 1m5s | Loss: 2.360 | Acc: 38.158% (8694/2278 178/391 \n",
      "179==========>.............]  Step: 369ms | Tot: 1m5s | Loss: 2.359 | Acc: 38.172% (8746/2291 179/391 \n",
      "180==========>.............]  Step: 354ms | Tot: 1m6s | Loss: 2.360 | Acc: 38.164% (8793/2304 180/391 \n",
      "181==========>.............]  Step: 367ms | Tot: 1m6s | Loss: 2.361 | Acc: 38.143% (8837/2316 181/391 \n",
      "182==========>.............]  Step: 366ms | Tot: 1m6s | Loss: 2.361 | Acc: 38.122% (8881/2329 182/391 \n",
      "183==========>.............]  Step: 362ms | Tot: 1m7s | Loss: 2.361 | Acc: 38.136% (8933/2342 183/391 \n",
      "184==========>.............]  Step: 357ms | Tot: 1m7s | Loss: 2.361 | Acc: 38.167% (8989/2355 184/391 \n",
      "185==========>.............]  Step: 369ms | Tot: 1m7s | Loss: 2.362 | Acc: 38.142% (9032/2368 185/391 \n",
      "186==========>.............]  Step: 363ms | Tot: 1m8s | Loss: 2.363 | Acc: 38.126% (9077/2380 186/391 \n",
      "187==========>.............]  Step: 359ms | Tot: 1m8s | Loss: 2.362 | Acc: 38.131% (9127/2393 187/391 \n",
      "188==========>.............]  Step: 364ms | Tot: 1m8s | Loss: 2.362 | Acc: 38.148% (9180/2406 188/391 \n",
      "189===========>............]  Step: 367ms | Tot: 1m9s | Loss: 2.361 | Acc: 38.153% (9230/2419 189/391 \n",
      "190===========>............]  Step: 363ms | Tot: 1m9s | Loss: 2.363 | Acc: 38.113% (9269/2432 190/391 \n",
      "191===========>............]  Step: 359ms | Tot: 1m10s | Loss: 2.365 | Acc: 38.081% (9310/2444 191/391 \n",
      "192===========>............]  Step: 363ms | Tot: 1m10s | Loss: 2.366 | Acc: 38.045% (9350/2457 192/391 \n",
      "193===========>............]  Step: 365ms | Tot: 1m10s | Loss: 2.366 | Acc: 38.030% (9395/2470 193/391 \n",
      "194===========>............]  Step: 359ms | Tot: 1m11s | Loss: 2.367 | Acc: 38.011% (9439/2483 194/391 \n",
      "195===========>............]  Step: 366ms | Tot: 1m11s | Loss: 2.368 | Acc: 37.993% (9483/2496 195/391 \n",
      "196===========>............]  Step: 363ms | Tot: 1m11s | Loss: 2.368 | Acc: 37.982% (9529/2508 196/391 \n",
      "197===========>............]  Step: 365ms | Tot: 1m12s | Loss: 2.369 | Acc: 37.980% (9577/2521 197/391 \n",
      "198===========>............]  Step: 367ms | Tot: 1m12s | Loss: 2.367 | Acc: 38.041% (9641/2534 198/391 \n",
      "199===========>............]  Step: 364ms | Tot: 1m12s | Loss: 2.366 | Acc: 38.065% (9696/2547 199/391 \n",
      "200===========>............]  Step: 362ms | Tot: 1m13s | Loss: 2.365 | Acc: 38.074% (9747/2560 200/391 \n",
      "201===========>............]  Step: 360ms | Tot: 1m13s | Loss: 2.365 | Acc: 38.087% (9799/2572 201/391 \n",
      "202===========>............]  Step: 365ms | Tot: 1m14s | Loss: 2.365 | Acc: 38.103% (9852/2585 202/391 \n",
      "203===========>............]  Step: 356ms | Tot: 1m14s | Loss: 2.365 | Acc: 38.123% (9906/2598 203/391 \n",
      "204===========>............]  Step: 351ms | Tot: 1m14s | Loss: 2.366 | Acc: 38.109% (9951/2611 204/391 \n",
      "205============>...........]  Step: 363ms | Tot: 1m15s | Loss: 2.365 | Acc: 38.159% (10013/2624 205/391 \n",
      "206============>...........]  Step: 355ms | Tot: 1m15s | Loss: 2.366 | Acc: 38.137% (10056/2636 206/391 \n",
      "207============>...........]  Step: 361ms | Tot: 1m15s | Loss: 2.366 | Acc: 38.145% (10107/2649 207/391 \n",
      "208============>...........]  Step: 371ms | Tot: 1m16s | Loss: 2.365 | Acc: 38.202% (10171/2662 208/391 \n",
      "209============>...........]  Step: 367ms | Tot: 1m16s | Loss: 2.365 | Acc: 38.188% (10216/2675 209/391 \n",
      "210============>...........]  Step: 362ms | Tot: 1m16s | Loss: 2.364 | Acc: 38.203% (10269/2688 210/391 \n",
      "211============>...........]  Step: 361ms | Tot: 1m17s | Loss: 2.364 | Acc: 38.196% (10316/2700 211/391 \n",
      "212============>...........]  Step: 363ms | Tot: 1m17s | Loss: 2.364 | Acc: 38.211% (10369/2713 212/391 \n",
      "213============>...........]  Step: 366ms | Tot: 1m18s | Loss: 2.364 | Acc: 38.197% (10414/2726 213/391 \n",
      "214============>...........]  Step: 357ms | Tot: 1m18s | Loss: 2.365 | Acc: 38.190% (10461/2739 214/391 \n",
      "215============>...........]  Step: 358ms | Tot: 1m18s | Loss: 2.363 | Acc: 38.209% (10515/2752 215/391 \n",
      "216============>...........]  Step: 358ms | Tot: 1m19s | Loss: 2.363 | Acc: 38.202% (10562/2764 216/391 \n",
      "217============>...........]  Step: 362ms | Tot: 1m19s | Loss: 2.363 | Acc: 38.234% (10620/2777 217/391 \n",
      "218============>...........]  Step: 359ms | Tot: 1m19s | Loss: 2.363 | Acc: 38.238% (10670/2790 218/391 \n",
      "219============>...........]  Step: 359ms | Tot: 1m20s | Loss: 2.362 | Acc: 38.271% (10728/2803 219/391 \n",
      "220=============>..........]  Step: 360ms | Tot: 1m20s | Loss: 2.362 | Acc: 38.299% (10785/2816 220/391 \n",
      "221=============>..........]  Step: 361ms | Tot: 1m20s | Loss: 2.361 | Acc: 38.324% (10841/2828 221/391 \n",
      "222=============>..........]  Step: 363ms | Tot: 1m21s | Loss: 2.360 | Acc: 38.341% (10895/2841 222/391 \n",
      "223=============>..........]  Step: 360ms | Tot: 1m21s | Loss: 2.360 | Acc: 38.341% (10944/2854 223/391 \n",
      "224=============>..........]  Step: 357ms | Tot: 1m22s | Loss: 2.360 | Acc: 38.337% (10992/2867 224/391 \n",
      "225=============>..........]  Step: 363ms | Tot: 1m22s | Loss: 2.360 | Acc: 38.323% (11037/2880 225/391 \n",
      "226=============>..........]  Step: 352ms | Tot: 1m22s | Loss: 2.360 | Acc: 38.333% (11089/2892 226/391 \n",
      "227=============>..........]  Step: 354ms | Tot: 1m23s | Loss: 2.361 | Acc: 38.298% (11128/2905 227/391 \n",
      "228=============>..........]  Step: 355ms | Tot: 1m23s | Loss: 2.363 | Acc: 38.274% (11170/2918 228/391 \n",
      "229=============>..........]  Step: 356ms | Tot: 1m23s | Loss: 2.362 | Acc: 38.271% (11218/2931 229/391 \n",
      "230=============>..........]  Step: 360ms | Tot: 1m24s | Loss: 2.362 | Acc: 38.254% (11262/2944 230/391 \n",
      "231=============>..........]  Step: 368ms | Tot: 1m24s | Loss: 2.361 | Acc: 38.274% (11317/2956 231/391 \n",
      "232=============>..........]  Step: 361ms | Tot: 1m24s | Loss: 2.361 | Acc: 38.278% (11367/2969 232/391 \n",
      "233=============>..........]  Step: 360ms | Tot: 1m25s | Loss: 2.361 | Acc: 38.268% (11413/2982 233/391 \n",
      "234=============>..........]  Step: 354ms | Tot: 1m25s | Loss: 2.362 | Acc: 38.241% (11454/2995 234/391 \n",
      "235=============>..........]  Step: 361ms | Tot: 1m25s | Loss: 2.363 | Acc: 38.225% (11498/3008 235/391 \n",
      "236==============>.........]  Step: 359ms | Tot: 1m26s | Loss: 2.362 | Acc: 38.238% (11551/3020 236/391 \n",
      "237==============>.........]  Step: 364ms | Tot: 1m26s | Loss: 2.362 | Acc: 38.252% (11604/3033 237/391 \n",
      "238==============>.........]  Step: 361ms | Tot: 1m27s | Loss: 2.361 | Acc: 38.281% (11662/3046 238/391 \n",
      "239==============>.........]  Step: 361ms | Tot: 1m27s | Loss: 2.361 | Acc: 38.291% (11714/3059 239/391 \n",
      "240==============>.........]  Step: 358ms | Tot: 1m27s | Loss: 2.362 | Acc: 38.285% (11761/3072 240/391 \n",
      "241==============>.........]  Step: 355ms | Tot: 1m28s | Loss: 2.361 | Acc: 38.314% (11819/3084 241/391 \n",
      "242==============>.........]  Step: 360ms | Tot: 1m28s | Loss: 2.362 | Acc: 38.314% (11868/3097 242/391 \n",
      "243==============>.........]  Step: 368ms | Tot: 1m28s | Loss: 2.362 | Acc: 38.294% (11911/3110 243/391 \n",
      "244==============>.........]  Step: 362ms | Tot: 1m29s | Loss: 2.363 | Acc: 38.294% (11960/3123 244/391 \n",
      "245==============>.........]  Step: 358ms | Tot: 1m29s | Loss: 2.362 | Acc: 38.313% (12015/3136 245/391 \n",
      "246==============>.........]  Step: 366ms | Tot: 1m29s | Loss: 2.362 | Acc: 38.329% (12069/3148 246/391 \n",
      "247==============>.........]  Step: 371ms | Tot: 1m30s | Loss: 2.362 | Acc: 38.300% (12109/3161 247/391 \n",
      "248==============>.........]  Step: 362ms | Tot: 1m30s | Loss: 2.361 | Acc: 38.306% (12160/3174 248/391 \n",
      "249==============>.........]  Step: 361ms | Tot: 1m31s | Loss: 2.360 | Acc: 38.328% (12216/3187 249/391 \n",
      "250==============>.........]  Step: 361ms | Tot: 1m31s | Loss: 2.360 | Acc: 38.344% (12270/3200 250/391 \n",
      "251==============>.........]  Step: 363ms | Tot: 1m31s | Loss: 2.359 | Acc: 38.362% (12325/3212 251/391 \n",
      "252===============>........]  Step: 378ms | Tot: 1m32s | Loss: 2.359 | Acc: 38.362% (12374/3225 252/391 \n",
      "253===============>........]  Step: 362ms | Tot: 1m32s | Loss: 2.360 | Acc: 38.365% (12424/3238 253/391 \n",
      "254===============>........]  Step: 356ms | Tot: 1m32s | Loss: 2.359 | Acc: 38.380% (12478/3251 254/391 \n",
      "255===============>........]  Step: 363ms | Tot: 1m33s | Loss: 2.360 | Acc: 38.379% (12527/3264 255/391 \n",
      "256===============>........]  Step: 358ms | Tot: 1m33s | Loss: 2.359 | Acc: 38.370% (12573/3276 256/391 \n",
      "257===============>........]  Step: 356ms | Tot: 1m33s | Loss: 2.360 | Acc: 38.354% (12617/3289 257/391 \n",
      "258===============>........]  Step: 352ms | Tot: 1m34s | Loss: 2.360 | Acc: 38.336% (12660/3302 258/391 \n",
      "259===============>........]  Step: 354ms | Tot: 1m34s | Loss: 2.361 | Acc: 38.330% (12707/3315 259/391 \n",
      "260===============>........]  Step: 357ms | Tot: 1m34s | Loss: 2.360 | Acc: 38.332% (12757/3328 260/391 \n",
      "261===============>........]  Step: 363ms | Tot: 1m35s | Loss: 2.360 | Acc: 38.323% (12803/3340 261/391 \n",
      "262===============>........]  Step: 362ms | Tot: 1m35s | Loss: 2.360 | Acc: 38.302% (12845/3353 262/391 \n",
      "263===============>........]  Step: 356ms | Tot: 1m36s | Loss: 2.360 | Acc: 38.308% (12896/3366 263/391 \n",
      "264===============>........]  Step: 362ms | Tot: 1m36s | Loss: 2.360 | Acc: 38.284% (12937/3379 264/391 \n",
      "265===============>........]  Step: 361ms | Tot: 1m36s | Loss: 2.361 | Acc: 38.269% (12981/3392 265/391 \n",
      "266===============>........]  Step: 368ms | Tot: 1m37s | Loss: 2.361 | Acc: 38.264% (13028/3404 266/391 \n",
      "267================>.......]  Step: 363ms | Tot: 1m37s | Loss: 2.362 | Acc: 38.264% (13077/3417 267/391 \n",
      "268================>.......]  Step: 365ms | Tot: 1m37s | Loss: 2.361 | Acc: 38.264% (13126/3430 268/391 \n",
      "269================>.......]  Step: 367ms | Tot: 1m38s | Loss: 2.361 | Acc: 38.249% (13170/3443 269/391 \n",
      "270================>.......]  Step: 359ms | Tot: 1m38s | Loss: 2.362 | Acc: 38.252% (13220/3456 270/391 \n",
      "271================>.......]  Step: 360ms | Tot: 1m38s | Loss: 2.361 | Acc: 38.273% (13276/3468 271/391 \n",
      "272================>.......]  Step: 362ms | Tot: 1m39s | Loss: 2.361 | Acc: 38.290% (13331/3481 272/391 \n",
      "273================>.......]  Step: 361ms | Tot: 1m39s | Loss: 2.360 | Acc: 38.284% (13378/3494 273/391 \n",
      "274================>.......]  Step: 362ms | Tot: 1m40s | Loss: 2.361 | Acc: 38.267% (13421/3507 274/391 \n",
      "275================>.......]  Step: 362ms | Tot: 1m40s | Loss: 2.361 | Acc: 38.270% (13471/3520 275/391 \n",
      "276================>.......]  Step: 360ms | Tot: 1m40s | Loss: 2.361 | Acc: 38.261% (13517/3532 276/391 \n",
      "277================>.......]  Step: 351ms | Tot: 1m41s | Loss: 2.361 | Acc: 38.262% (13566/3545 277/391 \n",
      "278================>.......]  Step: 357ms | Tot: 1m41s | Loss: 2.361 | Acc: 38.267% (13617/3558 278/391 \n",
      "279================>.......]  Step: 350ms | Tot: 1m41s | Loss: 2.361 | Acc: 38.259% (13663/3571 279/391 \n",
      "280================>.......]  Step: 354ms | Tot: 1m42s | Loss: 2.361 | Acc: 38.251% (13709/3584 280/391 \n",
      "281================>.......]  Step: 361ms | Tot: 1m42s | Loss: 2.361 | Acc: 38.234% (13752/3596 281/391 \n",
      "282================>.......]  Step: 366ms | Tot: 1m42s | Loss: 2.360 | Acc: 38.259% (13810/3609 282/391 \n",
      "283=================>......]  Step: 359ms | Tot: 1m43s | Loss: 2.361 | Acc: 38.254% (13857/3622 283/391 \n",
      "284=================>......]  Step: 354ms | Tot: 1m43s | Loss: 2.361 | Acc: 38.248% (13904/3635 284/391 \n",
      "285=================>......]  Step: 359ms | Tot: 1m44s | Loss: 2.362 | Acc: 38.226% (13945/3648 285/391 \n",
      "286=================>......]  Step: 382ms | Tot: 1m44s | Loss: 2.362 | Acc: 38.246% (14001/3660 286/391 \n",
      "287=================>......]  Step: 364ms | Tot: 1m44s | Loss: 2.362 | Acc: 38.249% (14051/3673 287/391 \n",
      "288=================>......]  Step: 365ms | Tot: 1m45s | Loss: 2.362 | Acc: 38.273% (14109/3686 288/391 \n",
      "289=================>......]  Step: 363ms | Tot: 1m45s | Loss: 2.362 | Acc: 38.260% (14153/3699 289/391 \n",
      "290=================>......]  Step: 370ms | Tot: 1m45s | Loss: 2.361 | Acc: 38.295% (14215/3712 290/391 \n",
      "291=================>......]  Step: 358ms | Tot: 1m46s | Loss: 2.362 | Acc: 38.268% (14254/3724 291/391 \n",
      "292=================>......]  Step: 370ms | Tot: 1m46s | Loss: 2.362 | Acc: 38.260% (14300/3737 292/391 \n",
      "293=================>......]  Step: 366ms | Tot: 1m46s | Loss: 2.361 | Acc: 38.273% (14354/3750 293/391 \n",
      "294=================>......]  Step: 362ms | Tot: 1m47s | Loss: 2.361 | Acc: 38.271% (14402/3763 294/391 \n",
      "295=================>......]  Step: 375ms | Tot: 1m47s | Loss: 2.362 | Acc: 38.249% (14443/3776 295/391 \n",
      "296=================>......]  Step: 359ms | Tot: 1m48s | Loss: 2.362 | Acc: 38.255% (14494/3788 296/391 \n",
      "297=================>......]  Step: 350ms | Tot: 1m48s | Loss: 2.362 | Acc: 38.252% (14542/3801 297/391 \n",
      "298=================>......]  Step: 367ms | Tot: 1m48s | Loss: 2.362 | Acc: 38.255% (14592/3814 298/391 \n",
      "299==================>.....]  Step: 356ms | Tot: 1m49s | Loss: 2.361 | Acc: 38.289% (14654/3827 299/391 \n",
      "300==================>.....]  Step: 364ms | Tot: 1m49s | Loss: 2.361 | Acc: 38.281% (14700/3840 300/391 \n",
      "301==================>.....]  Step: 362ms | Tot: 1m49s | Loss: 2.360 | Acc: 38.292% (14753/3852 301/391 \n",
      "302==================>.....]  Step: 358ms | Tot: 1m50s | Loss: 2.361 | Acc: 38.273% (14795/3865 302/391 \n",
      "303==================>.....]  Step: 355ms | Tot: 1m50s | Loss: 2.361 | Acc: 38.276% (14845/3878 303/391 \n",
      "304==================>.....]  Step: 361ms | Tot: 1m50s | Loss: 2.361 | Acc: 38.271% (14892/3891 304/391 \n",
      "305==================>.....]  Step: 363ms | Tot: 1m51s | Loss: 2.361 | Acc: 38.263% (14938/3904 305/391 \n",
      "306==================>.....]  Step: 361ms | Tot: 1m51s | Loss: 2.362 | Acc: 38.256% (14984/3916 306/391 \n",
      "307==================>.....]  Step: 363ms | Tot: 1m52s | Loss: 2.362 | Acc: 38.276% (15041/3929 307/391 \n",
      "308==================>.....]  Step: 358ms | Tot: 1m52s | Loss: 2.362 | Acc: 38.263% (15085/3942 308/391 \n",
      "309==================>.....]  Step: 360ms | Tot: 1m52s | Loss: 2.362 | Acc: 38.238% (15124/3955 309/391 \n",
      "310==================>.....]  Step: 367ms | Tot: 1m53s | Loss: 2.362 | Acc: 38.251% (15178/3968 310/391 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311==================>.....]  Step: 360ms | Tot: 1m53s | Loss: 2.361 | Acc: 38.276% (15237/3980 311/391 \n",
      "312==================>.....]  Step: 375ms | Tot: 1m53s | Loss: 2.361 | Acc: 38.301% (15296/3993 312/391 \n",
      "313==================>.....]  Step: 435ms | Tot: 1m54s | Loss: 2.361 | Acc: 38.281% (15337/4006 313/391 \n",
      "314===================>....]  Step: 386ms | Tot: 1m54s | Loss: 2.361 | Acc: 38.286% (15388/4019 314/391 \n",
      "315===================>....]  Step: 384ms | Tot: 1m55s | Loss: 2.361 | Acc: 38.296% (15441/4032 315/391 \n",
      "316===================>....]  Step: 375ms | Tot: 1m55s | Loss: 2.362 | Acc: 38.284% (15485/4044 316/391 \n",
      "317===================>....]  Step: 375ms | Tot: 1m55s | Loss: 2.363 | Acc: 38.266% (15527/4057 317/391 \n",
      "318===================>....]  Step: 368ms | Tot: 1m56s | Loss: 2.363 | Acc: 38.252% (15570/4070 318/391 \n",
      "319===================>....]  Step: 363ms | Tot: 1m56s | Loss: 2.363 | Acc: 38.254% (15620/4083 319/391 \n",
      "320===================>....]  Step: 364ms | Tot: 1m56s | Loss: 2.362 | Acc: 38.279% (15679/4096 320/391 \n",
      "321===================>....]  Step: 364ms | Tot: 1m57s | Loss: 2.363 | Acc: 38.279% (15728/4108 321/391 \n",
      "322===================>....]  Step: 364ms | Tot: 1m57s | Loss: 2.362 | Acc: 38.293% (15783/4121 322/391 \n",
      "323===================>....]  Step: 357ms | Tot: 1m57s | Loss: 2.361 | Acc: 38.303% (15836/4134 323/391 \n",
      "324===================>....]  Step: 358ms | Tot: 1m58s | Loss: 2.361 | Acc: 38.308% (15887/4147 324/391 \n",
      "325===================>....]  Step: 366ms | Tot: 1m58s | Loss: 2.362 | Acc: 38.303% (15934/4160 325/391 \n",
      "326===================>....]  Step: 366ms | Tot: 1m59s | Loss: 2.362 | Acc: 38.298% (15981/4172 326/391 \n",
      "327===================>....]  Step: 363ms | Tot: 1m59s | Loss: 2.362 | Acc: 38.305% (16033/4185 327/391 \n",
      "328===================>....]  Step: 362ms | Tot: 1m59s | Loss: 2.364 | Acc: 38.279% (16071/4198 328/391 \n",
      "329===================>....]  Step: 359ms | Tot: 2m158ms | Loss: 2.363 | Acc: 38.303% (16130/4211 329/391 \n",
      "330====================>...]  Step: 364ms | Tot: 2m523ms | Loss: 2.362 | Acc: 38.310% (16182/4224 330/391 \n",
      "331====================>...]  Step: 358ms | Tot: 2m881ms | Loss: 2.363 | Acc: 38.305% (16229/4236 331/391 \n",
      "332====================>...]  Step: 354ms | Tot: 2m1s | Loss: 2.362 | Acc: 38.305% (16278/4249 332/391 \n",
      "333====================>...]  Step: 360ms | Tot: 2m1s | Loss: 2.363 | Acc: 38.288% (16320/4262 333/391 \n",
      "334====================>...]  Step: 358ms | Tot: 2m1s | Loss: 2.363 | Acc: 38.291% (16370/4275 334/391 \n",
      "335====================>...]  Step: 360ms | Tot: 2m2s | Loss: 2.364 | Acc: 38.260% (16406/4288 335/391 \n",
      "336====================>...]  Step: 357ms | Tot: 2m2s | Loss: 2.364 | Acc: 38.265% (16457/4300 336/391 \n",
      "337====================>...]  Step: 355ms | Tot: 2m3s | Loss: 2.365 | Acc: 38.258% (16503/4313 337/391 \n",
      "338====================>...]  Step: 362ms | Tot: 2m3s | Loss: 2.364 | Acc: 38.256% (16551/4326 338/391 \n",
      "339====================>...]  Step: 354ms | Tot: 2m3s | Loss: 2.364 | Acc: 38.265% (16604/4339 339/391 \n",
      "340====================>...]  Step: 359ms | Tot: 2m4s | Loss: 2.364 | Acc: 38.258% (16650/4352 340/391 \n",
      "341====================>...]  Step: 364ms | Tot: 2m4s | Loss: 2.366 | Acc: 38.242% (16692/4364 341/391 \n",
      "342====================>...]  Step: 369ms | Tot: 2m4s | Loss: 2.365 | Acc: 38.263% (16750/4377 342/391 \n",
      "343====================>...]  Step: 387ms | Tot: 2m5s | Loss: 2.365 | Acc: 38.290% (16811/4390 343/391 \n",
      "344====================>...]  Step: 368ms | Tot: 2m5s | Loss: 2.365 | Acc: 38.293% (16861/4403 344/391 \n",
      "345====================>...]  Step: 443ms | Tot: 2m6s | Loss: 2.365 | Acc: 38.302% (16914/4416 345/391 \n",
      "346=====================>..]  Step: 406ms | Tot: 2m6s | Loss: 2.364 | Acc: 38.322% (16972/4428 346/391 \n",
      "347=====================>..]  Step: 408ms | Tot: 2m6s | Loss: 2.364 | Acc: 38.329% (17024/4441 347/391 \n",
      "348=====================>..]  Step: 417ms | Tot: 2m7s | Loss: 2.363 | Acc: 38.342% (17079/4454 348/391 \n",
      "349=====================>..]  Step: 395ms | Tot: 2m7s | Loss: 2.363 | Acc: 38.346% (17130/4467 349/391 \n",
      "350=====================>..]  Step: 420ms | Tot: 2m8s | Loss: 2.363 | Acc: 38.330% (17172/4480 350/391 \n",
      "351=====================>..]  Step: 460ms | Tot: 2m8s | Loss: 2.363 | Acc: 38.337% (17224/4492 351/391 \n",
      "352=====================>..]  Step: 438ms | Tot: 2m8s | Loss: 2.363 | Acc: 38.343% (17276/4505 352/391 \n",
      "353=====================>..]  Step: 404ms | Tot: 2m9s | Loss: 2.364 | Acc: 38.334% (17321/4518 353/391 \n",
      "354=====================>..]  Step: 429ms | Tot: 2m9s | Loss: 2.365 | Acc: 38.328% (17367/4531 354/391 \n",
      "355=====================>..]  Step: 495ms | Tot: 2m10s | Loss: 2.364 | Acc: 38.330% (17417/4544 355/391 \n",
      "356=====================>..]  Step: 407ms | Tot: 2m10s | Loss: 2.364 | Acc: 38.338% (17470/4556 356/391 \n",
      "357=====================>..]  Step: 381ms | Tot: 2m11s | Loss: 2.364 | Acc: 38.325% (17513/4569 357/391 \n",
      "358=====================>..]  Step: 369ms | Tot: 2m11s | Loss: 2.364 | Acc: 38.305% (17553/4582 358/391 \n",
      "359=====================>..]  Step: 364ms | Tot: 2m11s | Loss: 2.365 | Acc: 38.286% (17593/4595 359/391 \n",
      "360=====================>..]  Step: 360ms | Tot: 2m12s | Loss: 2.364 | Acc: 38.277% (17638/4608 360/391 \n",
      "361======================>.]  Step: 356ms | Tot: 2m12s | Loss: 2.365 | Acc: 38.262% (17680/4620 361/391 \n",
      "362======================>.]  Step: 431ms | Tot: 2m12s | Loss: 2.365 | Acc: 38.264% (17730/4633 362/391 \n",
      "363======================>.]  Step: 391ms | Tot: 2m13s | Loss: 2.365 | Acc: 38.260% (17777/4646 363/391 \n",
      "364======================>.]  Step: 366ms | Tot: 2m13s | Loss: 2.365 | Acc: 38.251% (17822/4659 364/391 \n",
      "365======================>.]  Step: 370ms | Tot: 2m14s | Loss: 2.365 | Acc: 38.253% (17872/4672 365/391 \n",
      "366======================>.]  Step: 363ms | Tot: 2m14s | Loss: 2.365 | Acc: 38.256% (17922/4684 366/391 \n",
      "367======================>.]  Step: 376ms | Tot: 2m14s | Loss: 2.365 | Acc: 38.243% (17965/4697 367/391 \n",
      "368======================>.]  Step: 382ms | Tot: 2m15s | Loss: 2.365 | Acc: 38.247% (18016/4710 368/391 \n",
      "369======================>.]  Step: 383ms | Tot: 2m15s | Loss: 2.365 | Acc: 38.256% (18069/4723 369/391 \n",
      "370======================>.]  Step: 388ms | Tot: 2m16s | Loss: 2.365 | Acc: 38.237% (18109/4736 370/391 \n",
      "371======================>.]  Step: 390ms | Tot: 2m16s | Loss: 2.365 | Acc: 38.229% (18154/4748 371/391 \n",
      "372======================>.]  Step: 400ms | Tot: 2m16s | Loss: 2.365 | Acc: 38.241% (18209/4761 372/391 \n",
      "373======================>.]  Step: 414ms | Tot: 2m17s | Loss: 2.366 | Acc: 38.233% (18254/4774 373/391 \n",
      "374======================>.]  Step: 403ms | Tot: 2m17s | Loss: 2.366 | Acc: 38.242% (18307/4787 374/391 \n",
      "375======================>.]  Step: 395ms | Tot: 2m18s | Loss: 2.366 | Acc: 38.244% (18357/4800 375/391 \n",
      "376======================>.]  Step: 390ms | Tot: 2m18s | Loss: 2.366 | Acc: 38.238% (18403/4812 376/391 \n",
      "377=======================>]  Step: 396ms | Tot: 2m18s | Loss: 2.366 | Acc: 38.248% (18457/4825 377/391 \n",
      "378=======================>]  Step: 384ms | Tot: 2m19s | Loss: 2.366 | Acc: 38.248% (18506/4838 378/391 \n",
      "379=======================>]  Step: 385ms | Tot: 2m19s | Loss: 2.367 | Acc: 38.219% (18541/4851 379/391 \n",
      "380=======================>]  Step: 387ms | Tot: 2m19s | Loss: 2.368 | Acc: 38.201% (18581/4864 380/391 \n",
      "381=======================>]  Step: 381ms | Tot: 2m20s | Loss: 2.369 | Acc: 38.162% (18611/4876 381/391 \n",
      "382=======================>]  Step: 382ms | Tot: 2m20s | Loss: 2.369 | Acc: 38.152% (18655/4889 382/391 \n",
      "383=======================>]  Step: 379ms | Tot: 2m21s | Loss: 2.369 | Acc: 38.151% (18703/4902 383/391 \n",
      "384=======================>]  Step: 377ms | Tot: 2m21s | Loss: 2.369 | Acc: 38.157% (18755/4915 384/391 \n",
      "385=======================>]  Step: 371ms | Tot: 2m21s | Loss: 2.370 | Acc: 38.135% (18793/4928 385/391 \n",
      "386=======================>]  Step: 375ms | Tot: 2m22s | Loss: 2.371 | Acc: 38.123% (18836/4940 386/391 \n",
      "387=======================>]  Step: 377ms | Tot: 2m22s | Loss: 2.371 | Acc: 38.122% (18884/4953 387/391 \n",
      "388=======================>]  Step: 371ms | Tot: 2m22s | Loss: 2.372 | Acc: 38.106% (18925/4966 388/391 \n",
      "389=======================>]  Step: 373ms | Tot: 2m23s | Loss: 2.373 | Acc: 38.080% (18961/4979 389/391 \n",
      "390=======================>]  Step: 376ms | Tot: 2m23s | Loss: 2.374 | Acc: 38.079% (19009/4992 390/391 \n",
      " [========================>]  Step: 297ms | Tot: 2m24s | Loss: 2.375 | Acc: 38.064% (19032/5000 391/391 \n"
     ]
    }
   ],
   "source": [
    "net_p = net\n",
    "net_p_train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34.94"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_acc(net_p.eval())\n",
    "#net_p_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ADVERSARIAL ATTACK ON PRUNNED\n",
    "import foolbox as fb\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "\n",
    "\n",
    "\n",
    "fmodel = fb.PyTorchModel(net_p.eval(), bounds=(-3.3,3.3))\n",
    "images, labels = ep.astensors(*samples(fmodel, dataset=\"cifar100\", batchsize=16))\n",
    "\n",
    "attack = fb.attacks.LinfPGD()\n",
    "epsilons = [0.0, 0.001, 0.005, 0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy for Pruned Network (epsilon, accuracy)\n",
      "eps: 0.0 || acc: 0.0\n",
      "eps: 0.001 || acc: 0.0\n",
      "eps: 0.005 || acc: 0.0\n",
      "eps: 0.01 || acc: 0.0\n",
      "eps: 0.03 || acc: 0.0\n",
      "eps: 0.1 || acc: 0.0\n",
      "eps: 0.3 || acc: 0.0\n",
      "eps: 0.5 || acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# calculate and report the robust accuracy\n",
    "print('Robust accuracy for Pruned Network (epsilon, accuracy)')\n",
    "robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "for eps, acc in zip(epsilons, robust_accuracy):\n",
    "    print('eps:',eps, '||','acc:', acc.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy for Pre-Trained Network (epsilon, accuracy)\n",
      "eps: 0.0 || acc: 0.125\n",
      "eps: 0.001 || acc: 0.125\n",
      "eps: 0.005 || acc: 0.0625\n",
      "eps: 0.01 || acc: 0.0625\n",
      "eps: 0.03 || acc: 0.0\n",
      "eps: 0.1 || acc: 0.0\n",
      "eps: 0.3 || acc: 0.0\n",
      "eps: 0.5 || acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "###ADVERSARIAL ATTACK ON Pre-Trained\n",
    "cfg = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "net = AlexNet(cfg).to(device)\n",
    "PATH_pre = './pretrained_alex.pth'\n",
    "net_dict = torch.load(PATH_pre, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(net_dict['net'])\n",
    "\n",
    "fmodel = fb.PyTorchModel(net.eval(), bounds=(0,1))\n",
    "images, labels = ep.astensors(*samples(fmodel, dataset=\"cifar100\", batchsize=16))\n",
    "\n",
    "attack = fb.attacks.LinfPGD()\n",
    "epsilons = [0.0, 0.001, 0.005, 0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "\n",
    "# calculate and report the robust accuracy\n",
    "robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "print('Robust accuracy for Pre-Trained Network (epsilon, accuracy)')\n",
    "for eps, acc in zip(epsilons, robust_accuracy):\n",
    "    print('eps:',eps, '||','acc:', acc.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "0.0 0.9375\n",
      "0.001 0.25\n",
      "0.01 0.0\n",
      "0.03 0.0\n",
      "0.1 0.0\n",
      "0.3 0.0\n",
      "0.5 0.0\n",
      "1.0 0.0\n",
      "0.0 0.9375\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.001 0.25\n",
      "[0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001\n",
      " 0.001 0.001 0.001 0.001]\n",
      "0.01 0.0\n",
      "[0.01000002 0.01000002 0.01000002 0.01000002 0.01000002 0.01000002\n",
      " 0.01000002 0.01000002 0.01000002 0.01000002 0.01000002 0.01000002\n",
      " 0.01000002 0.01000002 0.01000002 0.01000002]\n",
      "0.03 0.0\n",
      "[0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03\n",
      " 0.03 0.03]\n",
      "0.1 0.0\n",
      "[0.10000002 0.10000002 0.10000002 0.10000002 0.10000002 0.10000002\n",
      " 0.10000002 0.10000002 0.10000002 0.10000002 0.10000002 0.10000002\n",
      " 0.10000002 0.10000002 0.10000002 0.10000002]\n",
      "0.3 0.0\n",
      "[0.30000004 0.30000004 0.30000004 0.30000004 0.30000004 0.30000004\n",
      " 0.30000004 0.30000004 0.30000004 0.30000004 0.30000004 0.30000004\n",
      " 0.30000004 0.30000004 0.30000004 0.30000004]\n",
      "0.5 0.0\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "1.0 0.0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from foolbox.attacks import LinfPGD\n",
    "\n",
    "model = models.resnet18(pretrained=True).eval()\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n",
    "\n",
    "# get data and test the model\n",
    "# wrapping the tensors with ep.astensors is optional, but it allows\n",
    "# us to work with EagerPy tensors in the following\n",
    "images, labels = ep.astensors(*samples(fmodel, dataset=\"imagenet\", batchsize=16))\n",
    "print(accuracy(fmodel, images, labels))\n",
    "\n",
    "# apply the attack\n",
    "attack = LinfPGD()\n",
    "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
    "advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "\n",
    "# calculate and report the robust accuracy\n",
    "robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "for eps, acc in zip(epsilons, robust_accuracy):\n",
    "    print(eps, acc.item())\n",
    "\n",
    "# we can also manually check this\n",
    "for eps, advs_ in zip(epsilons, advs):\n",
    "    print(eps, accuracy(fmodel, advs_, labels))\n",
    "    # but then we also need to look at the perturbation sizes\n",
    "    # and check if they are smaller than eps\n",
    "    print((advs_ - images).norms.linf(axis=(1, 2, 3)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = []\n",
    "\n",
    "for i in [2, 6, 10, 13, 16]:\n",
    "    orig_size.append(net_corr.features[i].bias.shape[0])\n",
    "\n",
    "for i in [1, 4]:\n",
    "    orig_size.append(net_corr.classifier[i].bias.shape[0])\n",
    "    \n",
    "orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_corr, prune_ratio = order_and_ratios(imp_order_corr, 0.2)\n",
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load(PATH_corr)\n",
    "net_corr.load_state_dict(net_dict['net'])\n",
    "net_p = pruner(net_corr, order_corr, prune_ratio, orig_size, net_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p.eval()), cal_acc(net_corr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def net_p_train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_p.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = optim.Adam(net_p.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        print(batch_idx)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc =  100.*correct/total\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), acc, correct, total))\n",
    "        if acc > 50.0:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net_p': net_p.state_dict(),\n",
    "                'best_p_acc': acc\n",
    "            }\n",
    "            if not os.path.isdir('net_p_checkpoint'):\n",
    "                os.mkdir('net_p_checkpoint')\n",
    "            torch.save(state, './net_p_checkpoint/ckpt'+str(51)+'.pth')\n",
    "            best_p_acc = acc\n",
    "def net_p_test(epoch):\n",
    "    global best_p_acc\n",
    "    global prune_iter\n",
    "    net_p.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_p(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > 50.0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_p': net_p.state_dict(),\n",
    "            'best_p_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('net_p_checkpoint'):\n",
    "            os.mkdir('net_p_checkpoint')\n",
    "        torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')\n",
    "        best_p_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    net_p_train(epoch)\n",
    "    net_p_test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load correlated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = torch.load('./net_p_checkpoint/ckpt1.pth')\n",
    "net_p.load_state_dict(net_dict['net_p'])\n",
    "best_p_acc = net_dict['best_p_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "#     imp_order_p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_p.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 6, 10, 13, 16, 1, 4]:\n",
    "    print(l_index)\n",
    "    if(l_index != 1 and l_index != 4):\n",
    "        nlist = cal_importance_conv(net_p, l_index)\n",
    "    else:\n",
    "        nlist = cal_importance_linear(net_p, l_index)\n",
    "    imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruned network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network '''\n",
    "orig_size = []\n",
    "for i in [2, 6, 10, 13, 16]:\n",
    "    orig_size.append(net_p.features[i].bias.shape[0])\n",
    "for i in [1, 4]:\n",
    "    orig_size.append(net_p.classifier[i].bias.shape[0])\n",
    "orig_size = np.array(orig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network '''\n",
    "order_p, prune_ratio = order_and_ratios(imp_order_p, 0.1)\n",
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Correlated network pruning '''\n",
    "net_p1 = pruner(net_p, order_p, prune_ratio, orig_size, net_type=1)\n",
    "\n",
    "print(\"Accs:\", cal_acc(net_p1.eval()), cal_acc(net_p.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network saving '''\n",
    "net_p = net_p1\n",
    "\n",
    "print('Saving..')\n",
    "state = {\n",
    "    'net_p': net_p.state_dict(),\n",
    "    'best_p_acc': cal_acc(net_p.eval())\n",
    "}\n",
    "if not os.path.isdir('net_p_checkpoint'):\n",
    "    os.mkdir('net_p_checkpoint')\n",
    "torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network loading '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(1)+\".pkl\", 'rb') as f:\n",
    "#     cfg_p1 = pickle.load(f)\n",
    "    \n",
    "# net_p = AlexNet(cfg_p1).to(device)\n",
    "# PATH = './net_p_checkpoint/ckpt'+str(1)+'.pth'\n",
    "# net_p.load_state_dict(torch.load(PATH)['net_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_acc(net_p.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPS calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with torch.cuda.device(0):\n",
    "flops, params = get_model_complexity_info(net_p, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity 60% Pruned: ', flops))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity Original: ', flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
